{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# III.1 Variable Aleatoria Conjunta\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Consideremos el **espacio de probabilidad** $\\mathscr{E}(\\Omega, \\mathscr{F}, P)$. Sobre él podemos definir no solo una, como ya hemos hecho, sino varias **variables aleatorias** $X_1, X_2, \\ldots X_N$:\n",
    "\n",
    "\n",
    "$$\\begin{align*}\n",
    "X_n:  \\mathscr{\\Omega} & \\longmapsto \\mathbb{R}\\\\\n",
    "   \\alpha & \\longmapsto X_i(\\alpha)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "   \n",
    "Cada una de estas variables aleatorias puede ser discreta, continua o mixta. Al estar definidas sobre el mismo espacio de probabilidad, los distintos sucesos descritos por cada una de las variables aleatorias tendrán, en general una dependencia probabilística entre sí, y los valores de estas variables tendrán entre sí relaciones no deterministas. Caben dos casos extremos:\n",
    "\n",
    "* **Dependencia funcional**: en cuyo caso las variables aleatorias satisfacen una ecuación sin componente aleatoria alguna\n",
    "* **Independencia**: si por cómo están construidas las variables aleatorias los sucesos que se representan con cada una de ellas son independientes dos a dos con los de todas las demás. En tal caso, **los valores de unas variables aleatorias no ofrecen información alguna sobre las restantes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El espacio aleatorio sobre el que se definen las variables aleatorias puede corresponder a un experimento compuesto (recordar lo visto en el Bloque I), de forma que cada una de las variables aleatorias se define sobre cada uno de los espacios que lo componen.\n",
    "\n",
    "$$\\mathscr{E}(\\Omega, \\mathscr{F}, P) = \\mathscr{E}_1(\\Omega_1, \\mathscr{F}_1, P_1) \\times \\ldots \\times \\mathscr{E}_N(\\Omega_N, \\mathscr{F}_N, P_N)$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "X_n:  \\mathscr{\\Omega_n} & \\longmapsto \\mathbb{R}\\\\\n",
    "   \\alpha & \\longmapsto X_n(\\alpha)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "Por supuesto, si los experimentos marginales (componentes del experimento compuesto) son independientes, también lo serán las variables aleatorias definidas sobre cada uno de ellos.\n",
    "\n",
    "Adviértase, no obstante, que <u>no es necesario definir varias variables aleatorias sobre experimentos compuestos</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consideremos algunos ejemplos:\n",
    "\n",
    "* En el lanzamiento de un dado, número de cara ($X_1$) y su doble ($X_2$)\n",
    "* Lanzamiento simultáneo de una moneda $X_1$ y un dado $X_2$\n",
    "* Lanzamiento sucesivo de una moneda cinco veces: $X_1, X_2, \\ldots X_5$\n",
    "* Lazamiento de un dado ($X_1$) e instante de llamada ($X_2$) de mis padres entre 22h y 23h (con certeza de que lo harán en esa franja)\n",
    "* Dos muestras sucesivas, espaciadas 1 mseg: $X_1$ y $X_2$, de la señal de voz del profesor dando la clase\n",
    "* El valor de cierre del índice Ibex cinco días seguidos:  $X_1, X_2, \\ldots X_5$\n",
    "* Estatura ($X_1$), mpeso $X_2$ y edad en años $X_3$ de una persona cogida al azar\n",
    "* En el lanzamiento de un dado, cara ($X_1$) y acierto si es par ($X_2$)\n",
    "\n",
    "Reflexionar sobre los siguientes aspectos:\n",
    "\n",
    "* Carácter continuo, discreto o mixto de las variables aleatorias\n",
    "* Dependencia probabilística, funcional e independencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vectores aleatorios\n",
    "\n",
    "Es habitual reunir todas las variables aleatorias como componentes de un vector aleatorio *n-dimensional*.\n",
    "\n",
    "$$\\mathbf X = (X_1, \\ldots X_N)^t$$\n",
    "\n",
    "Cada una de las variables aleatorias $X_n$, componentes del vector aleatorio $\\mathbf X$, tiene una **caracterización probabilística marginal**, es decir, una a una por separado, descrita como se ha visto en los temas de variable aleatoria:\n",
    "\n",
    "* Funciones de distribución de probabilidad $F_{X_n}(x)$, que pueden expresarse como tablas con las funciones acumuladaa de probabilidad en el caso discreto, $F_{X_n}(x_i)$\n",
    "* Funciones de densidad de probabilidad $f_{X_n}(x)$, que pueden expresarse mediante funciones de masa de probabilidad $f_{X_n}(x_i)$ en el caso discreto\n",
    "* Momentos de orden $k$ no centrados, $E(X_n^k)$, y centrados en la media, $E((X_n-\\eta_{X_n})^k)$. En particular:\n",
    "  * Media: $\\eta_{X_n}=E(X_n)$\n",
    "  * Valor cuadrático medio: $E(X_n^2)$\n",
    "  * Varianza: $\\sigma_{X_n}^2=Var(X_n)=E((X_n-\\eta_{X_n})^2)= E(X_n^2) - \\eta_{X_n}^2$\n",
    "  \n",
    "El objetivo de este tema es, como veremos, considerar la **caracterización probabilística conjunta** del vector aleatorio $\\mathbf X$ o, dicho de otro modo, de las variables aleatorias $X_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Secuencias aleatorias y procesos estocásticos\n",
    "\n",
    "Un caso interesante es cuando el número de variables aleatorias crece de forma indefinida y ya no es posible representarlo mediante un vector de dimensión finito, dando lugar a una **secuencia aleatoria**, $X[n]$. Se trata de una secuencia indefinida, en general sin comienzo y sin final, de muestras aleatorias, cada una de ellas representable mediante una variable aleatoria.\n",
    "\n",
    "Si en vez de una secuencia de muestras aleatorias consideramos una función que toma valores aleatorios, no sobre un índice discreto sino sobre todo el eje real (por ejemplo, un eje de tiempo), estamos ante un **proceso estocástico**, $X(t)$, en el que el valor que toma la función en cada instante es una variable aleatoria. Estrictamente, un proceso estocástico asigna a cada resultado de un experimento aleatorio una función real de variable real del conjunto $\\mathcal{F}(\\mathbb{R}, \\mathbb{R})$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "X:  \\mathscr{\\Omega} & \\longmapsto \\mathcal{F}(\\mathbb{R}, \\mathbb{R})\\\\\n",
    "   \\alpha & \\longmapsto X(t, \\alpha)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "   \n",
    "Adviértase que, si se fija el resultado $\\alpha$ estamos ante una función determinista mientras que, si se fija el instante $t$ estamos ante una variable aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para cada índice de la muestra o instante de tiempo puede hacerse una **caracterización marginal de la secuencia o proceso**:\n",
    "\n",
    "$$F_X(x; n) \\quad f_X(x; n) \\quad\\qquad F_X(x; t) \\quad f_X(x; t)$$\n",
    "\n",
    "Igualmente con los **momentos**. Por ejemplo:\n",
    "\n",
    "$$\\eta_X[n] = E(X[n]) \\qquad \\sigma_X^2[n]=Var(X[n]) = E(X^2[n]) - \\eta_X^2[n]$$\n",
    "\n",
    "$$\\eta_X(t) = E(X(t)) \\qquad \\sigma_X^2(t)=Var(X(t)) = E(X^2(t)) - \\eta_X^2(t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Secuencias y procesos estacionarios\n",
    "\n",
    "Si la caracterización probabilística de la secuencia o proceso es **temporalmente invariante**, esto es, si tal caracterización no depende de cuándo se establece el origen de tiempos o, altenativamente, de desplazar temporalmente la secuencia o el proceso, estamos ante una **secuencia o proceso estacionario en sentido estricto**. Dicho de otra manera, las caracterizaciones probabilísticas de $X(t)$ y de $X(t+t_0)$ son idénticas para cualquier $t_0 \\in \\mathbb{R}$. Igualmente pasa con $X[n]$ y $X[n+n_0]$ para cualquier $n_0 \\in \\mathbb{Z}$.\n",
    "\n",
    "Lógicamente la estacionariedad en sentido estricto conlleva que las funciones de distribución de probabilidad, las funciones de densidad de probabilidad y, en el caso discreto, las funciones de masa de probabilidad, así como todos los momentos, no varían a lo largo del tiempo o el índice de la muestra\n",
    "\n",
    "$$F_X(x; n)\\equiv F_X(x) \\quad f_X(x; n)\\equiv f_X(x) \\quad\\qquad F_X(x; t)\\equiv F_X(x) \\quad f_X(x; t)\\equiv f_X(x)$$\n",
    "\n",
    "$$ \\eta_X[n] \\equiv \\eta_X \\quad \\sigma_X^2[n] \\equiv \\sigma_X^2 \\quad\\qquad \\eta_X(t) \\equiv \\eta_X \\quad \\sigma_X^2(t) \\equiv \\sigma_X^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Secuencias y procesos ergódicos\n",
    "\n",
    "En principio, la estimación de parámetros estadísticos de secuencias y procesos aleatorios requiere promediar en los resultados del espacio probabilístico suyacente. Por ejemplo, si $N$ es el número de realizaciones, la media de un proceso $X(t)$ será:\n",
    "\n",
    "$$\\eta_X(t) = \\lim\\limits_{N \\to \\infty} \\frac{1}{N}\\sum_{\\alpha_i \\in\\Omega} X(t, \\alpha_i)$$\n",
    "\n",
    "Frecuentemente esto no resulta práctico, pues no es posible repetir muchas veces el experimento para obtener cada una de las funciones y promediarlas. \n",
    "\n",
    "Sin embargo, **para muchas secuencias o procesos estacionarios** es posible estimar los parámetros estadísticos a partir de una única realización. En tal caso la secuencia o proceso es **ergódica**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La idea es que es posible (no siempre) obtener tales estimaciones promediando a lo largo del tiempo o índice, en vez de hacerlo a lo largo de las distintas realizaciones. La ergodicidad es una propiedad muy útil que vincula la estadística con las secuencias y procesos estacionarios. Pero es también matemáticamente compleja, y no vamos a profundizar en ella. \n",
    "\n",
    "Por ejemplo, la media de un proceso estacionario y ergódico podría calcularse:\n",
    "\n",
    "$$\\eta_X = \\lim\\limits_{T \\to \\infty} \\frac{1}{2T}\\int_{-T}^T X(t) dt \\qquad \\eta_X = \\lim\\limits_{N \\to \\infty} \\frac{1}{2N+1}\\sum_{-N}^N X[n]$$\n",
    "\n",
    "En términos prácticos, <u>si la secuencia o proceso es ergódico, podemos muestrearlo temporalmente para estimar sus parámetros estadísticos</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Señales aleatorias y ruido\n",
    "\n",
    "Cuando todas las variables aleatorias correspondientes tanto a las muestras de la secuencia aleatoria como a los instantes del proceso estocástico son **independientes** entre sí, decimos que la secuencia o el proceso es **blanco**. Una señal o secuencia aleatoria e indeseada suele denominarse **ruido**. El **ruido blanco** es una secuencia aleatoria, $N[n]$, o proceso estocástico, $N(t)$, en el que todas las variables aleatorias de las muestras o instantes son independientes entre sí.\n",
    "\n",
    "En ingeniería de telecomunicación y electrónica, así como en otras áreas de la ciencia y la técnica, es muy habitual considerar procesos estocásticos que modelan **señales en ruido**, donde $S(t)$ puede ser determinista desconocida o aleatoria, según sea el modelo que se aplique, respectivamente, clásico (basado en la *verosimilitud*) o bayesiano. El ruido es, por definición, siempre aleatorio:\n",
    "\n",
    "$$X(t) = S(t) + N(t)$$\n",
    "\n",
    "Al muestrear tales procesos, con $X[n] = S(n\\Delta T_s)$, siendo $T_s$ el periodo de muestreo, obtenemos secuencias aleatorias de señal en ruido:\n",
    "\n",
    "$$X[n] = S[n] + N[n]$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Es también habitual trabajar sólo con las muestras correspondientes a un intervalo, no con toda la secuencia. En tales casos, se trabaja con un número finito y predeterminado de muestras que se ordenan en vectores aleatorios de dimensión $N$ (no confundir con el ruido), resultando el siguiente modelo:\n",
    "\n",
    "$$\\mathbf X = \\mathbf S + \\mathbf N$$\n",
    "\n",
    "Los problemas más habituales ante los que nos encontramos son los siguientes:\n",
    "\n",
    "* **Filtrado** consiste en estimar la señal $S$ a pertir de las medidas ruidosas $X$. \n",
    "* **Clasificación** consiste en identificar la señal $S_i$ entre un conjunto finito de señales posibles a partir de medidas ruidosas.\n",
    "* **Predicción** consiste en estimar la señal $S$ en un instante o muestra futuro a partir de medidas ruidosas anteriores.\n",
    "* **Interpolación** consiste en estimar la señal $S$ en instantes en los que no disponemos de medida a partir de medidas ruidosas tanto anteriores como posteriores\n",
    "\n",
    "En todos los casos, la señal $S$ puede considerarse determinista o aleatoria, dando lugar a métodos clásicos o bayesionospara resolver los problemas mencionados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dos variables aleatorias o vector aleatorio bidimensional\n",
    "\n",
    "Ya hemos visto la **caracterización marginal** de vectores y secuencias aleatorias y de procesos estocásticos. Sin embargo esto solo muestra vistas parciales de los mismos, una por cada variable aleatoria que podemos extraer. Hemos de añadir el conocimiento de la dependencia probabilística entre las variables aleatorias que los componen, lo que nos lleva a la **caracterización conjunta de vectores y secuencias aleatorias y de procesos estocásticos**. Es ilustrativo para ello estudiar en detalle el caso de dos variables aleatoria o vector aleatorio bidimensional, $\\mathbf V = (X, Y)^t$, que tiene una fácil representación gráfica en el plano mediante un **diagrama de dispersión** de sus realizaciones.\n",
    "\n",
    "REPRESENTAR DIAGRAMAS DE DISPERSIÖN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ya sabemos que las variables aleatorias, $X$ e $Y$ las definimos sobre un espacio de probabilidad subyacente $\\mathscr{E}(\\Omega, \\mathscr{F}, P)$. Podemos definir sobre el plano $X \\times Y$ conjuntos que serán sucesos de $\\mathscr{F}$. Por ejemplo: \n",
    "\n",
    "* $\\{X \\leq x\\}$ es el semiplano que se extiende a la izquierda de la línea vertical $X = x$, y es un suceso de $\\mathscr{F}$.\n",
    "* $\\{Y \\leq y\\}$ es el semiplano que se extiende por debajo de la línea horizontal $Y = y$, y también un suceso de $\\mathscr{F}$.\n",
    "* $\\{X\\leq x, Y\\leq y\\} = \\{X \\leq x\\} \\bigcap \\{Y \\leq y\\}$ es, por tanto, el cuadrante que queda a la izquierda de la línea vertical $X = x$ y por debajo de la línea horizontal $Y = y$, y es también un suceso de $\\mathscr{F}$.\n",
    "\n",
    "Adviértase que una variable aleatoria proporciona, por lo general, una **descripción parcial del espacio de probabilidad** subyacente, que, cuando tenemos definidas varias, llamamos **caracterización marginal**. Esto resulta evidente sin más que pensar, por ejemplo, en un experimento compuesto, al que asignamos una variable aleatoria diferente a cada experimento componente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Función de Distribución Conjunta de Probabilidad\n",
    "\n",
    "Considérese un vector aleatorio $(X, Y)$ o, equivalentemente, dos variables aleatorias, $X$ e $Y$. Sus **funciones de distribución marginales** son:\n",
    "\n",
    "* $F_X(x) = P(\\{X\\leq x\\}) \\quad -\\infty \\leq x \\leq \\infty$\n",
    "* $F_Y(y) = P(\\{Y\\leq y\\}) \\quad -\\infty \\leq y \\leq \\infty$\n",
    "\n",
    "El conocimiento de $F_X$ y $F_Y$ no es, en general, suficiente para poder obtener las probabilidades de sucesos del tipo $\\{X\\leq x, Y\\leq y\\}$. Necesitamos para ello conocer una función diferente, la **función de distribución conjunta de probabilidad**, $F_{XY}(x,y)$ del vector aleatorio $(X, Y)$ es:\n",
    "\n",
    "$$F_{XY}(x,y) = P\\left(\\{X\\leq x, Y\\leq y\\}\\right)$$\n",
    "\n",
    "Podemos considerar que  $F_{XY}(x,y)$ proporciona la **masa de probabilidad acumulada en el cuadrante** que queda a la izquierda de la línea vertical $X = x$ y por debajo de la línea horizontal $Y = y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las principales propiedades de $F_{XY}(x,y)$ son las siguientes, y se entienden intuitivamente sin más que considerar cómo se distribuye la masa de probabilidad en el plano:\n",
    "\n",
    "* $F_{XY}(-\\infty, y) = F_{XY}(x, -\\infty) = 0 \\quad \\forall x,y \\in \\mathbb{R}$, pues no hay masa de probabilidad acumulada \n",
    "* $F_{XY}(\\infty, \\infty) = 1$, pues corresponde a la totalidad de la masa de probabilidad\n",
    "* $F_{XY}(x,y)$ es **no decreciente** tanto en $x$ como $y$. EN los puntos donde se localicen masas discretas de probabilidad $F_{XY}$ muestra discontinuidades de salto. En el resto, es continua. \n",
    "* El suceso $\\{x_1 < X \\leq x_2, Y \\leq y\\}$ es una franja vertical limitada por $X=x_1$ (sin incluir) y $X=x_2$ (incluido) cuya masa de probabilidad es $F_{XY}(x_2,y)-F_{XY}(x_1,y)$\n",
    "* El suceso $\\{X \\leq x, y_1 < Y \\leq y_2\\}$ es una franja horizontal limitada por $Y=y_1$ (sin incluir) e $Y=y_2$ (incluido) cuya masa de probabilidad es $F_{XY}(x,y_2)-F_{XY}(x,y_1)$\n",
    "* El suceso $\\{x_1 <X\\leq x_2 , \\leq x, y_1 < Y \\leq y_2\\}$ es un rectángulo limitado por $X=x_1$ (sin incluir, $X=x_2$ (incluido), $Y=y_1$ (sin incluir) e $Y=y_2$ (incluido), cuya masa de probabilidad es $F_{XY}(x_2,y_2)-F_{XY}(x_1,y_2)-F_{XY}(x_2,y_1)+F_{XY}(x_1,y_1)$\n",
    "\n",
    "Si $X$ e $Y$ son variables aleatorias discretas, puede trabajarse con una tabla con la función de probabilidad acumulada: $F_{XY}(x_i, y_j) \\quad i,j \\in \\mathbb{Z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Relación entre las funciones de distribución marginales y la conjunta\n",
    "\n",
    "**Es inmediato obtener las funciones de distribución marginales a partir de la conjunta**, sin más que entender que la ausencia de una variable aleatoria significa que puede darse cualquiera de ellos y, por tanto, debe cubrirse por completo el eje correspondiente:\n",
    "\n",
    "* $F_X(x) = F_{XY}(x,\\infty)$\n",
    "* $F_Y(y) = F_{XY}(\\infty , y)$\n",
    "\n",
    "Sin embargo, como ya se ha dicho, **no es posible, en general, obtener la distribución conjunta a partir de las marginales**. La excepción es cuando **ambas variables aleatorias son independientes** pues, en tal caso:\n",
    "\n",
    "$$F_{XY}(x,y) = P\\left(\\{X \\leq x\\} \\bigcap \\{Y \\leq y\\}\\right)=P(\\{X \\leq x\\})P(\\{Y \\leq y\\})= F_X(x)F_Y(y)$$\n",
    "\n",
    "Por tanto, si $X$ e $Y$ son **variables aleatorias independientes**: $F_{XY}(x,y) = F_X(x)F_Y(y) \\quad \\forall x,y \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Función de masa de probabilidad conjunta\n",
    "\n",
    "Concentrémonos ahora en el caso en que tanto $X$ como $Y$ son discretas. Ya hemos visto que la función de distribución conjunta podemos expresarla mediante una tabla de probabilidades acumuladas indexada por las posiciones donde se localizan las masa de probabilidad. Es conveniente repasar la forma de hacerlo con una sola variable aleatoria.\n",
    "\n",
    "$$F_{XY}(x_i,y_j) \\quad i,j \\in \\mathbb{Z}$$\n",
    "\n",
    "Podemos fácilmente obtener las masa de probabilidad en cada localización, e introducir con ello la **función de masa de probabilidad conjunta** del vector aleatorio $(X,Y)$ o de ambas variables aleatorias $X$ e $Y$:\n",
    "\n",
    "$$p_{XY}(x_i, y_j) \\equiv P(X=x, Y=y) = F_{XY}(x_i, y_j) - F_{XY}(x_{i-1}, y_j) -  F_{XY}(x_i, y_{j-1}) + F_{XY}(x_{i-1}, y_{j-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La **tabla de probabilidades acumuladas** correspondiente a la función de distribución conjunta se obtiene a partir de la función de masa de probabilidad conjunta:\n",
    "\n",
    "$$F_{XY}(x_i, y_j) = \\sum_{m=-\\infty}^i\\sum_{n=-\\infty}^j p_{XY}(x_m, y_n)$$\n",
    "\n",
    "Es posible obtener una expresión recursiva para $F_{XY}(x_i,y_j)$ sin más que despejar:\n",
    "\n",
    "$$F_{XY}(x_i, y_j) = \\left( F_{XY}(x_{i-1}, y_j) +  F_{XY}(x_i, y_{j-1}) - F_{XY}(x_{i-1}, y_{j-1})\\right) + p_{XY}(x_i, y_j) $$\n",
    "\n",
    "Adviértase que, por tratarse de probabilidades, $p_{XY}(x_i, y_j) \\geq 0$ y, también:\n",
    "\n",
    "$$F_{XY}(\\infty, \\infty) = 1 \\implies \\sum_{i=-\\infty}^\\infty\\sum_{j=-\\infty}^\\infty p_{XY}(x_i, y_j) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Relación entre las funciones de masa de probabilidad marginales y la conjunta\n",
    "\n",
    "Considerando que $F_X(x_i)=F_{XY}(x_i,\\infty)$ y que $p_X(x_i) = F_X(x_i)-F_X(x_{i-1})$, pueden obtenerse las funciones de masa de probabilidad marginales a partir de la conjunta:\n",
    "\n",
    "$$p_X(x_i) = \\sum_{m=-\\infty}^i\\sum_{n=-\\infty}^\\infty p_{XY}(x_m, y_n) - \\sum_{m=-\\infty}^{i-1}\\sum_{n=-\\infty}^\\infty p_{XY}(x_m, y_n) = \\sum_{j=-\\infty}^\\infty p_{XY}(x_i, y_j)$$\n",
    "\n",
    "Igualmente,\n",
    "\n",
    "$$p_Y(y_j) = \\sum_{i=-\\infty}^\\infty p_{XY}(x_i, y_j)$$\n",
    "\n",
    "En general, **la función de masa de probabilidad conjunta no puede obtenerse a partir de las funciones marginales**. Sin embargo, **si ambas variables son independientes**:\n",
    "\n",
    "$$p_{XY}(x_i, y_j) = P(X=x_i, Y=y_j) = p_X(x_i)p_Y(y_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ejemplo: caracterización conjunta de dos variables aleatorias discretas independientes\n",
    "\n",
    "Considérese el lanzamiento simultáneo de un dado trucado y de una moneda trucada. Representamos con la variable aleatoria $X_1$ los resultados del dado y con $X_2$ los de la moneda ($0$ es cara y $1$ es cruz). Las funciones de masa de probabilidad marginales son como sigue:\n",
    "\n",
    "$$\n",
    "X_1 \\begin{array}{c|cccccc|c}\n",
    "  x_{1_i} & 1 & 2 & 3 & 4 & 5 & 6 &  \\\\ \n",
    "  \\hline\n",
    "    p_{X_1}(x_i) & \\frac{1}{24} & \\frac{5}{24} & \\frac{5}{24} & \\frac{3}{24} & \\frac{4}{24} & \\frac{6}{24} & \\sum_i p_X(x_i)=1\n",
    " \\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_2 \\begin{array}{c|cc|c}\n",
    "x_{2_i} & 0 & 1\\\\\n",
    "\\hline\n",
    "p_{X_2}(x_{2_i}) & \\frac{11}{24} & \\frac{13}{24} & \\sum_i p_X(x_i) = 1\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Considerando que ambos lanzamientos son independientes, resulta la siguiente función de masa de probabilidad conjunta. Téngase en cuenta que, <u>para cualesquiera otrso valores de las variables aleatorias no presentes en la tabla, la masa de probbailidad es cero</u>.\n",
    "\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c|c}\n",
    "p_{X_1X_2}(x_{1_i}, x_{2_j}) & 0 & 1 & p_{X_1}(x_{1_i}) = \\sum_j p_{X_1X_2}(x_{1_i}x_{2_j})\\\\\n",
    "\\hline\n",
    "1 & \\frac{1}{24}\\frac{11}{24} = \\frac{11}{24^2} & \\frac{1}{24}\\frac{13}{24} = \\frac{13}{24^2} & \\frac{1}{24}\\\\\n",
    "\\hline\n",
    "2 & \\frac{5}{24}\\frac{11}{24} = \\frac{55}{24^2} & \\frac{5}{24}\\frac{13}{24} = \\frac{65}{24^2} & \\frac{5}{24}\\\\\n",
    "\\hline\n",
    "3 & \\frac{5}{24}\\frac{11}{24} = \\frac{55}{24^2} & \\frac{5}{24}\\frac{13}{24}  = \\frac{65}{24^2} & \\frac{5}{24}\\\\\n",
    "\\hline\n",
    "4 & \\frac{3}{24}\\frac{11}{24} = \\frac{33}{24^2} & \\frac{3}{24}\\frac{13}{24}  = \\frac{39}{24^2} & \\frac{3}{24}\\\\\n",
    "\\hline\n",
    "5 & \\frac{4}{24}\\frac{11}{24} = \\frac{44}{24^2}  & \\frac{4}{24}\\frac{13}{24} = \\frac{52}{24^2}  & \\frac{4}{24}\\\\\n",
    "\\hline\n",
    "6 & \\frac{6}{24}\\frac{11}{24} = \\frac{66}{24^2} & \\frac{6}{24}\\frac{13}{24} = \\frac{78}{24^2}  & \\frac{6}{24}\\\\\n",
    "\\hline\n",
    "p_{X_2}(x_{2_j}) = \\sum_i p_{X_1X_2}(x_{1_i}x_{2_j}) & \\frac{11}{24} & \\frac{13}{24} & 1\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La tabla con la función acumulada de probabilidad se obtiene de forma inmediata con la expresión recursiva ya vista:\n",
    "\n",
    "$$F_{X_1X_2}(x_{1_i}x_{2_j})= \\left(F_{X_1X_2}(x_{1_{i-1}}x_{2_{j}})+F_{X_1X_2}(x_{1_{i}}x_{2_{j-1}})- F_{X_1X_2}(x_{1_{i-1}}x_{2_{j-1}})\\right) + p_{X_1X_2}(x_{1_i}x_{2_j})$$\n",
    "\n",
    "Aunque, por supuesto, también podemos simplemente acumular probabilidades:\n",
    "\n",
    "$$F_{X_1X_2}(x_{1_i}, x_{2_j}) = \\sum_{x_1=1}^{x_{1_i}}\\sum_{x_2=0}^{x_{2_j}} p_{X_1X_2}(x_1, x_2)$$\n",
    "\n",
    "Adviértase también que las funciones de distribución marginales se obtienen también de la tabla:\n",
    "\n",
    "* $F_{X_1}(x_{1_i}) = F_{X_1X_2}(x_{1_i}, \\infty) = F_{X_1X_2}(x_{1_i}, 1) = F_{X_1X_2}(x_{1_i}, 2)\\ldots$\n",
    "* $F_{X_2}(x_{2_j}) = F_{X_1X_2}(\\infty, x_{2_j}) = F_{X_1X_2}(6, x_{2_j}) = F_{X_1X_2}(7, x_{2_j})\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c|c|c}\n",
    "F_{X_1X_2}(x_{1_i}, x_{2_j}) & -1 & 0 & 1 & 2\\\\\n",
    "\\hline\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\hline\n",
    "1 & 0 & \\frac{11}{24^2} & \\frac{11}{24^2} + \\frac{13}{24^2} = \\frac{24}{24^2} & \\frac{24}{24^2} = \\frac{1}{24}\\\\\n",
    "\\hline\n",
    "2 & 0 &\\frac{11}{24^2} + \\frac{55}{24^2} = \\frac{66}{24^2} & \\left(\\frac{24}{24^2} + \\frac{66}{24^2} - \\frac{11}{24^2}\\right) + \\frac{65}{24^2} = \\frac{144}{24^2} & \\frac{144}{24^2} = \\frac{6}{24}\\\\\n",
    "\\hline\n",
    "3 & 0 & \\frac{66}{24^2} + \\frac{55}{24^2} = \\frac{121}{24^2} & \\left(\\frac{144}{24^2} + \\frac{121}{24^2}- \\frac{66}{24^2}\\right) + \\frac{65}{24^2} = \\frac{264}{24^2} & \\frac{264}{24^2} =  \\frac{11}{24}\\\\\n",
    "\\hline\n",
    "4 & 0 & \\frac{121}{24^2} + \\frac{33}{24^2} = \\frac{154}{24^2} & \\left(\\frac{264}{24^2} + \\frac{154}{24^2} - \\frac{121}{24^2}\\right) + \\frac{39}{24^2} = \\frac{336}{24^2} & \\frac{336}{24^2} = \\frac{14}{24}\\\\\n",
    "\\hline\n",
    "5 & 0 & \\frac{154}{24^2} + \\frac{44}{24^2} = \\frac{198}{24^2} & \\left(\\frac{336}{24^2} + \\frac{198}{24^2} - \\frac{154}{24^2}\\right) + \\frac{52}{24^2} = \\frac{432}{24^2} & \\frac{432}{24^2} = \\frac{18}{24}\\\\\n",
    "\\hline\n",
    "6 & 0 & \\frac{198}{24^2} + \\frac{66}{24^2} = \\frac{264}{24^2} & \\left(\\frac{232}{24^2} + \\frac{264}{24^2} - \\frac{198}{24^2}\\right) + \\frac{78}{24^2} = \\frac{576}{24^2} & \\frac{576}{24^2} = \\frac{24}{24} =1\\\\\n",
    "\\hline\n",
    "7 & 0 & \\frac{264}{24^2} = \\frac{11}{24} & \\frac{576}{24^2} = \\frac{24}{24}= 1 & 1\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Caracterización conjunta de dos variables aleatorias discretas (caso general)\n",
    "\n",
    "Consideremos ahora que lanzamos un dado y una moneda, trucados de tal manera que hay dependencia entre sus resultados. En este caso nos tienen que dar tal dependencia en forma de una tabla con la función de masa de probabilidad conjunta, a partir de la cual podemos obtener las marginales (pero no al revés):\n",
    "\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c|c}\n",
    "p_{X_1X_2}(x_{1_i}, x_{2_j}) & 0 & 1 & p_{X_1}(x_{1_i}) = \\sum_j p_{X_1X_2}(x_{1_i}x_{2_j})\\\\\n",
    "\\hline\n",
    "1 & \\frac{1}{24} & 0 & \\frac{1}{24}\\\\\n",
    "\\hline\n",
    "2 & \\frac{2}{24} & \\frac{3}{24} & \\frac{5}{24}\\\\\n",
    "\\hline\n",
    "3 & \\frac{1}{24} & \\frac{4}{24} & \\frac{5}{24}\\\\\n",
    "\\hline\n",
    "4 & \\frac{2}{24} & \\frac{1}{24} & \\frac{3}{24}\\\\\n",
    "\\hline\n",
    "5 & \\frac{1}{24} & \\frac{3}{24} & \\frac{4}{24}\\\\\n",
    "\\hline\n",
    "6 & \\frac{4}{24} & \\frac{2}{24} & \\frac{6}{24}\\\\\n",
    "\\hline\n",
    "p_{X_2}(x_{2_j}) = \\sum_i p_{X_1X_2}(x_{1_i}x_{2_j}) & \\frac{11}{24} & \\frac{13}{24} & 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Adviértase que se ha mantenido para el ejemplo las mismas funciones de masa de probabilidad marginales que en el ejemplo anterior, si bien la función de masa de probabilidad conjunta es diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La función de masa de probabilidad acumulada es ahora:\n",
    "\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c|c}\n",
    "F_{X_1X_2}(x_{1_i}, x_{2_j}) & 0 & 1 & F_{X_1}(x_{1_i}) = F_{X_1X_2}(x_{1_i}, \\infty)\\\\\n",
    "\\hline\n",
    "1 & \\frac{1}{24} & \\frac{1}{24} & \\frac{1}{24}\\\\\n",
    "\\hline\n",
    "2 & \\frac{3}{24} & \\frac{6}{24} & \\frac{6}{24}\\\\\n",
    "\\hline\n",
    "3 & \\frac{4}{24} & \\frac{11}{24} & \\frac{11}{24}\\\\\n",
    "\\hline\n",
    "4 & \\frac{6}{24} & \\frac{14}{24} & \\frac{14}{24}\\\\\n",
    "\\hline\n",
    "5 & \\frac{7}{24} & \\frac{18}{24} & \\frac{18}{24}\\\\\n",
    "\\hline\n",
    "6 & \\frac{11}{24} & \\frac{24}{24}=1 & 1\\\\\n",
    "\\hline\n",
    "F_{X_2}(x_{2_j}) = F_{X_1X_2}(\\infty, x_{2_j}) & \\frac{11}{24} & 1 & \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Función de densidad de probabilidad conjunta\n",
    "\n",
    "Consideremos ahora que las variables aleatorias $X$ e $Y$ son continuas.Por igual razón que en el caso de una única variable aleatoria, en general, la masa de probabilidad en un punto es nula, $P(X=x, Y=y)=0$. Ello hace necesario recurrir a densidades de probabilidad, generalizanzdo lo visto en el caso de una variable aleatoria. La función de densidad de probabilidad conjunta del vector aleatorio $(X, Y)$, o de ambas variables aleatorias $X$ e $Y$, se define como sigue a partir de la función de distribución de probabilidad conjunta $F_{XY}(x,y)$:\n",
    "\n",
    "$$\n",
    "f_{XY}(x, y)=\\frac{\\partial^{2} F_{XY}(x, y)}{\\partial x \\partial y }\\geq 0\n",
    "$$\n",
    "\n",
    "que es positiva o nula por ser $F_{XY}(x,y)$ no decreciente. Además:\n",
    "\n",
    "$$\n",
    "F_{XY}(x, y)=\\int_{-\\infty}^{x} \\int_{-\\infty}^{y} f_{XY}(u, v) du dv\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si se ponen la derivadas parciales como límites, de forma semejante a como se hizo con una sola variable aleatoria:\n",
    "\n",
    "$$\n",
    "f_{XY}(x, y)=\\frac{\\partial^{2} F_{XY}(x, y)}{\\partial x \\partial y } = \\lim \\limits_{\\Delta x \\Delta y \\to 0} \\frac{F_{XY}(x+\\Delta x/2, y+\\Delta y/2)-F_{XY}(x-\\Delta x/2, y+\\Delta y/2)-F_{XY}(x+\\Delta x/2, y-\\Delta y/2)+F_{XY}(x-\\Delta x/2, y-\\Delta y/2)}{\\Delta x \\Delta y}= \\\\ \n",
    "\\lim \\limits_{\\Delta x \\Delta y \\to 0} \\frac{P\\left(\\{(x-\\Delta x/2) < x \\leq (x+\\Delta x/2)\\} \\bigcap \\{(y-\\Delta y/2) < y \\leq (y+\\Delta y/2)\\} \\right)}{\\Delta x \\Delta y} \n",
    "$$\n",
    "\n",
    "Donde se hace uso de un suceso rectangular infinitesimal centrado en $X=x$, $Y=y$. Por tanto:\n",
    "\n",
    "$$\n",
    "P\\left(\\{(x-\\Delta x/2) < x \\leq (x+\\Delta x/2)\\} \\bigcap \\{(y-\\Delta y/2) < y \\leq (y+\\Delta y/2)\\} \\right) \\approx f_{XY}(x, y) \\Delta x \\Delta y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Puede asimismo calcularse la masa de probabilidad continua en un dominio $D$ arbitrario:\n",
    "\n",
    "$$\n",
    "P\\{(x, y) \\in D\\}=\\int \\int_{D} f_{XY}(x, y) dx dy\n",
    "$$\n",
    "\n",
    "De la definición, se obtienen fácilmente las **funciones de densidad marginales**:\n",
    "\n",
    "$$\n",
    "f_{X}(x)=\\frac{\\partial F_{XY}(x, \\infty)}{\\partial x} = \\int_{-\\infty}^{\\infty} f_{XY}(x, y) dy\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_{Y}(y)=\\frac{\\partial F_{XY}(\\infty , y)}{\\partial y} = \\int_{-\\infty}^{\\infty} f_{XY}(x, y) dx\n",
    "$$\n",
    "\n",
    "Además, como siempre, la masa total de probabilidad es la unidad:\n",
    "\n",
    "$$\n",
    " F_{XY}(\\infty , \\infty) = 1 \\implies \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{XY}(x, y) d x d y=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distribuciones condicionadas\n",
    "\n",
    "La función de distribución conjunta puede resultar condicionada por un suceso $M$, sin más que extender lo visto para una variable aleatoria:\n",
    "\n",
    "$$\n",
    "F_{XY}(x, y | M)=P(\\{X \\leq x, Y \\leq y\\} \\ | \\ M)= \\frac{P(\\{X \\leq x, Y \\leq y\\} \\bigcap M)}{P(M)}\n",
    "$$\n",
    "\n",
    "Si las variables aleatorias $X$ e $Y$ son discretas, la función de masa de probabilidad conjunta condicionada por $M$ es:\n",
    "\n",
    "$$p_{XY}(x_i,y_j | M) = \\frac{P(\\{X=x_i,Y=y_j\\}\\bigcap M)}{P(M)} $$\n",
    "\n",
    "Adviértase que $\\sum_i\\sum_j p_{XY}(x_i,y_j | M) =1$\n",
    "\n",
    "Y en caso de que las variables aleatorias fueran continuas, la función de densidad de probabilidad conjunta condicionada es:\n",
    "\n",
    "$$\n",
    "f_{XY}(x, y | M)=\\frac{\\partial^{2} F_{XY}(x, y | M)}{\\partial x \\partial y }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos también definir la probabilidad de un suceso $M$ condicionada por un valor arbitrario $(x_i,y_j)$ del vector aleatorio $(X,Y)$. Para ello no hay más que advertir que $\\{(X=x_i, Y=y_j)\\}$ es un suceso del espacio muestral:\n",
    "\n",
    "$$P(M \\ | \\ \\{X=x_i,Y=y_j\\})= \\frac{P(M \\bigcap \\{X=x_i, Y=y_j\\})}{P(\\{X=x_i, Y=y_j\\})}=\\frac{P(M \\bigcap \\{X=x_i,Y=y_j\\})}{p_{XY}(x_i,y_j)}$$\n",
    "\n",
    "Mantendremos el habitual abuso de notación $P(M | x_i,y_j)\\equiv P(M | X=x_i,Y=y_j)$. Adviértase que $P(M | x_i,y_j) \\neq p_{XY}(x_i,y_j | M)$, como resulta del Teorema de Bayes.\n",
    "\n",
    "En el caso de variables aleatorias continuas, cuando el suceso condicionante es del tipo $\\{X=x, Y=y\\}$ es necesario considerar un suceso rectangular infinitesimal centrado en dicho punto, pues la masa de probabilidad puntual es nula. Las expresiones resultan semejantes a las del caso discreto, pero utilizando funciones de densidad en vez de funciones de masa de probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En particular, es posible obtener las masas de probabilidad de una variable aleatoria discreta condicionada por la otra:\n",
    "\n",
    "$$p_{X | Y}(x_i | y_j)=\\frac{p_{XY}(x_i,y_j)}{p_Y(y_j)}$$ \n",
    "\n",
    "$$p_{Y|X}(y_j | x_j)=\\frac{p_{XY}(x_i,y_j)}{p_X(x_i)}$$\n",
    "\n",
    "Adviértase que las probabilidades condicionadas proporcionan verosimilitudes dadas las observaciones. En particular:\n",
    "\n",
    "* Si la variable cuyos valores se observan es $X$ la verosimiltud de los valores de $Y$ es $L(y_j)=p_{X | Y}(x_i | y_j)$\n",
    "* Si la variable cuyos valores se observan es $Y$ la verosimiltud de los valores de $X$ es $L(x_i)=p_{Y | X}(y_j | x_i)$\n",
    "\n",
    "Por supuesto, en general $p_{X | Y}(x_i | y_j) \\neq p_{Y | X}(y_j | x_j)$, estando relacionados por el Teorema de Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "De forma semejante, si las variables aleatorias son continuas, las funciones de densidad de probabilidad condcionadas resultan:\n",
    "\n",
    "$$f_{X | Y}(x | y)=\\frac{f_{XY}(x,y)}{f_Y(y)}$$ \n",
    "\n",
    "$$f_{Y | X}(y | x)=\\frac{f_{XY}(x,y)}{f_X(x)}$$\n",
    "\n",
    "Igual que antes, las funciones de probabilidad condicionadas proporcionan verosimilitudes dadas las observaciones. En particular:\n",
    "\n",
    "* Si la variable cuyos valores se observan es $X$ la verosimiltud de los valores de $Y$ es $L(y)=p_{X | Y}(x | y)$\n",
    "* Si la variable cuyos valores se observan es $Y$ la verosimiltud de los valores de $X$ es $L(x)=p_{Y | X}(y | x)$\n",
    "\n",
    "De nuevo, en general $f_{X | Y}(x/y) \\neq f_{Y | X}(y | x)$, estando ambas relacionadas por el Teorema de Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Tomando las $p_{X_1X_2}(x_i,y_j)$ y $p_{X_1}(x_{1_i})$ del ejemplo anterior obtenemos $p_{X_2 | X_1}(x_{2_j} | x_{1_i})$:\n",
    "\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c|c}\n",
    "p_{X_2 | X_1}(x_{2_j} | x_{1_i}) & 0 & 1 & \\sum_j p_{X_1X_2}(x_{1_i}x_{2_j})\\\\\n",
    "\\hline\n",
    "1 & \\frac{\\frac{1}{24}}{\\frac{1}{24}}=1 & \\frac{0}{\\frac{1}{24}}=0 & 1\\\\\n",
    "\\hline\n",
    "2 & \\frac{\\frac{2}{24}}{\\frac{5}{24}}=\\frac{2}{5} & \\frac{\\frac{3}{24}}{\\frac{5}{24}}=\\frac{3}{5} & 1\\\\\n",
    "\\hline\n",
    "3 & \\frac{\\frac{1}{24}}{\\frac{5}{24}}=\\frac{1}{5} & \\frac{\\frac{4}{24}}{\\frac{5}{24}}=\\frac{4}{5} & 1\\\\\n",
    "\\hline\n",
    "4 & \\frac{\\frac{2}{24}}{\\frac{3}{24}}=\\frac{2}{3} & \\frac{\\frac{1}{24}}{\\frac{3}{24}}=\\frac{1}{3} & 1\\\\\n",
    "\\hline\n",
    "5 & \\frac{\\frac{1}{24}}{\\frac{4}{24}}=\\frac{1}{4} & \\frac{\\frac{3}{24}}{\\frac{4}{24}}=\\frac{3}{4} & 1\\\\\n",
    "\\hline\n",
    "6 & \\frac{\\frac{4}{24}}{\\frac{6}{24}}=\\frac{4}{6} & \\frac{\\frac{2}{24}}{\\frac{6}{24}}=\\frac{2}{6} & 1\\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Igualmente obtenemos $p_{X_1 | X_2}(x_{1_i} | x_{2_j})$ a partir de $p_{X_1X_2}(x_i,y_j)$ y $p_{X_2}(x_{2_j})$:\n",
    "\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c}\n",
    "p_{X_1 | X_2}(x_{1_i} | x_{2_j}) & 0 & 1\\\\\n",
    "\\hline\n",
    "1 & \\frac{\\frac{1}{24}}{\\frac{11}{24}}=\\frac{1}{11} & \\frac{0}{\\frac{13}{24}}=0\\\\\n",
    "\\hline\n",
    "2 & \\frac{\\frac{2}{24}}{\\frac{11}{24}}=\\frac{2}{11} & \\frac{\\frac{3}{24}}{\\frac{13}{24}}=\\frac{3}{13}\\\\\n",
    "\\hline\n",
    "3 & \\frac{\\frac{1}{24}}{\\frac{11}{24}}=\\frac{1}{11} & \\frac{\\frac{4}{24}}{\\frac{13}{24}}=\\frac{4}{13}\\\\\n",
    "\\hline\n",
    "4 & \\frac{\\frac{2}{24}}{\\frac{11}{24}}=\\frac{2}{11} & \\frac{\\frac{1}{24}}{\\frac{13}{24}}=\\frac{1}{13}\\\\\n",
    "\\hline\n",
    "5 & \\frac{\\frac{1}{24}}{\\frac{11}{24}}=\\frac{1}{11} & \\frac{\\frac{3}{24}}{\\frac{13}{24}}=\\frac{3}{13}\\\\\n",
    "\\hline\n",
    "6 & \\frac{\\frac{4}{24}}{\\frac{11}{24}}=\\frac{4}{11} & \\frac{\\frac{2}{24}}{\\frac{13}{24}}=\\frac{2}{13}\\\\\n",
    "\\hline\n",
    "\\sum_i p_{X_1 | X_2}(x_{1_i} | x_{2_j}) & 1 & 1 \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Teorema de la Probabilidad Total\n",
    "\n",
    "Es una extensión inmediata del visto para una única variable aleatoria. Consideremos una partición del espacio muestral y que conocemos las probabilidades *a priori* de los sucesos de la misma, $P(M_p)$:\n",
    "\n",
    "$$\n",
    "M_p \\in \\mathscr{P}(\\Omega) \\, p=1\\ldots N\\iff  \\begin{matrix}\n",
    "  \\bigcup_{p=1}^N M_p = \\Omega &  \\\\\n",
    "  M_p \\cap M_q = \\emptyset & p\\neq q  \n",
    " \\end{matrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Si conocemos las funciones de de distribución de probabilidad condicionada conjunta de un vector aleatorio $(X,Y)$  condicionadas por todos los sucesos de dicha partición $F_{XY}(x,y/M_p)$, es fácil obtener la función de distribución total (sin condicionar):\n",
    "\n",
    "$$F_{XY}(x,y) = \\sum_{p=1}^N F_{XY}(x,y | M_p)P(M_p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para el **caso discreto**, el Teorema de la Probabilidad Total puede expresarse a partir de las funciones de masa de probabilidad $p_{XY}(x_i,y_j | M_p)$, de modo que la función de masa de probabilidad total (sin condicionar) conjunta del vector aleatorio $(X,Y)$ resulta:\n",
    "\n",
    "$$p_{XY}(x_i,y_j) = \\sum_{p=1}^N p_{XY}(x_i,y_j | M_p)P(M_p)$$\n",
    "\n",
    "En el **caso continuo**, si conocemos las funciones de densidad de probabilidad condicionadas, $f_{XY}(x,y | M_p)$, la función de densidad de probabilidad total (sin condicionar) conjunta del vector aleatorio $(X,Y)$ es\n",
    "\n",
    "$$f_{XY}(x,y) = \\sum_{p=1}^N f_{XY}(x,y | M_p)P(M_p)$$.\n",
    "\n",
    "El Teorema de la Probabilidad Total permite obtener **mezclas (mixture) de distribuciones**. consistentes en la superposición ponderada de varias distribuciones, algo muy útil en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La probabilidad total también puede aplicarse de forma alternativa, si lo que se conocen son las probabilidades un suceso arbitrario $M$ condicionadas por cada uno de los valores posibles del vector aleatorio $(X,Y)$, esto es, si se conocen $P(M | x_i,y_j)\\equiv P(M | \\ \\{X=x_i,Y=y_j\\})$, en el caso discreto, y $P(M | x,y)\\equiv P(M | \\ \\{X=x,Y=y\\})$, en el caso continuo. \n",
    "\n",
    "En tal caso, la probabilidad total del suceso $M$ (sin condicionar) es, para el caso discreto,\n",
    "\n",
    "$$P(M) = \\sum_{i=-\\infty}^\\infty\\sum_{j=-\\infty}^\\infty P(M | x_i,y_j) p_{XY}(x_i,y_j)$$\n",
    "\n",
    "Y, para el caso continuo,\n",
    "\n",
    "$$P(M) = \\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty P(M | x,y) f_{XY}(x,y) dx dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En el caso del vector aleatorio discreto, si disponemos de la verosimilitud o función de masa de probabilidad condicionada de una variable (observación) respecto de la otra (condicionante) y de la función de masa de probabilidad marginal *apriori* de la variable condicionante, resulta la función de masa de probabilidad total marginal de las observaciones:\n",
    "\n",
    "$$p_{X}(x_i) = \\sum_j p_{X | Y}(x_i | y_j)p_{Y}(y_j)$$\n",
    "\n",
    "$$p_{Y}(y_i) = \\sum_i p_{Y | X}(y_j | x_i)p_{X}(x_i)$$\n",
    "\n",
    "Y si el vector aleatorio es continuo:\n",
    "\n",
    "$$f_{X}(x) = \\int_{-\\infty}^\\infty f_{X | Y}(x | y)f_{Y}(y) dy$$\n",
    "\n",
    "$$f_{Y}(y) = \\int_{-\\infty}^\\infty f_{Y | X}(y | x)f_{X}(x) dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Teorema de Bayes\n",
    "\n",
    "El Teorema de Bayes puede obtenerse en distinta formas, para lo cual es necesario recordar su definición y, si el suceso condicionante es un valor de una variable aleatoria continua y, por ello, tiene una masa de probabilidad infinitesimal, hay que considerar un rectángulo infinitesimal.\n",
    "\n",
    "Veamos una primera forma, que hace uso de la función e distribución conjunta y de un suceso arbitario $M$:\n",
    "\n",
    "$$P(M | \\ \\{X \\leq x, Y \\leq y\\}) = \\frac{F_{XY}(x,y | M)P(M)}{F_{XY}(x,y)}$$\n",
    "\n",
    "$$F_{XY}(x,y | M) = \\frac{P(M | \\ \\{X \\leq x, Y \\leq y\\} )F_{XY}(x,y)}{P(M)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En el **caso discreto**, es inmediato obtener el **Teorema de Bayes** a partir de la definición de función de masa de probablidad conjunta condicionada (recordar que $P(M | x_i,y_j)\\equiv P(M \\ \\ \\{X=x_i,Y=y_j\\})$):\n",
    "\n",
    "$$p_{XY}(x_i,y_j | M) = \\frac{P(M | x_i,y_j)p_{XY}(x_i,y_j)}{P(M)} \\iff P(M |x_i,y_j) = \\frac{p_{XY}(x_i,y_j/M)P(M)}{p_{XY}(x_i,y_j)}$$\n",
    "\n",
    "Con frecuencia, los denominadores de las expresiones anteriores se obtienen mediante el *Teorema de la Probabilidad Total*, pues tan sólo se conocen las probabilidades condicionadas:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_{XY}(x_i,y_j | M) &= \\frac{P(M | x_i,y_j)p_{XY}(x_i,y_j)}{\\sum_{i=-\\infty}^\\infty \\sum_{j=-\\infty}^\\infty P(M | x_i,y_j) p_{XY}(x_i,y_j)} \\\\ \n",
    "P(M | x_i,y_j) &= \\frac{p_{XY}(x_i,y_j | M)P(M)}{p_{XY}(x_i,y_j | M)P(M)+p_{XY}(x_i,y_j | \\overline M)(1-P(M))}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En el caso del vector aleatorio, si disponemos de la verosimilitud o función de masa de probabilidad condicionada de una variable (observación) respecto de la otra (condicionante) y de la función de masa de probabilidad marginal *a priori* de la variable condicionante, el Teorema de Bayes permite obtener la probabilidad *a posteriori* de la variable condicionante, dadas las observaciones:\n",
    "\n",
    "$$p_{X | Y}(x_i | y_j) = \\frac{p_{Y | X}(y_j | x_i)p_X(x_i)}{p_Y(y_j)}=\\frac{p_{Y | X}(y_j | x_i)p_X(x_i)}{\\sum_i p_{Y | X}(y_j |x_i)p_{X}(x_i)}$$\n",
    "\n",
    "\n",
    "$$p_{Y | X}(y_j | x_i) = \\frac{p_{X | Y}(x_i | y_j)p_Y(y_j)}{p_X(x_i)}=\\frac{p_{X | Y}(x_i | y_j)p_Y(y_j)}{\\sum_j p_{X | Y}(x_i | y_j)p_{Y}(y_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para el **caso continuo** las expresiones anteriores se adaptan sustituyendo funciones de masa de probabilidad por funciones de densidad y sumatorios por integrales:\n",
    "\n",
    "$$f_{XY}(x,y | M) = \\frac{P(M | x,y)p_{XY}(x,y)}{P(M)} \\iff P(M |x,y) = \\frac{f_{XY}(x,y | M)P(M)}{f_{XY}(x,y)}$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{XY}(x,y | M) &= \\frac{P(M | x,y)f_{XY}(x,y)}{\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty P(M | x,y) f_{XY}(x,y)dxdy} \\\\ \n",
    "P(M | x,y) &= \\frac{f_{XY}(x,y | M)P(M)}{f_{XY}(x,y | M)P(M)+f_{XY}(x,y | \\overline M)(1-P(M))}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$f_{X | Y}(x | y) = \\frac{f_{Y | X}(y | x)f_X(x)}{f_Y(y)}=\\frac{f_{Y | X}(y | x)f_X(x)}{\\int_{-\\infty}^\\infty f_{Y |f X}(y |x)f_{X}(x)dx}$$\n",
    "\n",
    "\n",
    "$$f_{Y | X}(y | x) = \\frac{f_{X | Y}(x | y)f_Y(y)}{f_X(x)}=\\frac{f_{X | Y}(x | y)f_Y(y)}{\\int_{-\\infty}^\\infty f_{X | Y}(x | y)f_{Y}(y)dy}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Consideremos en el ejemplo del lanzamiento del dado y moneda trucados y dependientes entre sí que las observaciones son las caras del dado $X_1$ y queremos hacer inferencia sobre el lado de la moneda $X_2$\n",
    "\n",
    "* Si sólo tenemos las probabilidades de las caras condicionadas por el lado de la moneda, $p_{X_1 | X_2}(x_{1_i} | x_{2_j})$ la inferencia se basa en el principio de máxima verosimilitud\n",
    "* Si se cuenta con las probabilidades *a priori* de los lados de la moneda, $p_{X_2}(x_{2_j})$, es posible obtener las probabilidades *a posteri* de  las mismas dadas las observaciones $p_{X_2/X_1}(x_{2_j} | x_{1_i})$. En este caso utilizamos el **principio de Máximo a Posteriori (MAP)** y podemos obtener las probabilidades de acierto y de error de cada elección. Adviértase que la búsqueda del **MAP** suele conllevar la división por la probabilidad total, que es la misma en todas las posibilidades y, por tanto, puede evitarse\n",
    "* Si se dispone de la probabilidad conjunta, se dispone de la máxima información probabilística para realizar inferenncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Verosimilitudes**: si sale 1, 4 ó 6 seleccionamos cara (0), y cruz (1) en los restantes casos: \n",
    "\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c}\n",
    "L(x_{2_j}) = p_{X_1 | X_2}(x_{1_i} | x_{2_j}) & 0 & 1\\\\\n",
    "\\hline\n",
    "1 & \\frac{1}{11} & 0\\\\\n",
    "\\hline\n",
    "2 & \\frac{2}{11} & \\frac{3}{13}\\\\\n",
    "\\hline\n",
    "3 & \\frac{1}{11} & \\frac{4}{13}\\\\\n",
    "\\hline\n",
    "4 & \\frac{2}{11} & \\frac{1}{13}\\\\\n",
    "\\hline\n",
    "5 & \\frac{1}{11} & \\frac{3}{13}\\\\\n",
    "\\hline\n",
    "6 & \\frac{4}{11} & \\frac{2}{13}\\\\\n",
    "\\hline\n",
    "\\sum_i p_{X_1 | X_2}(x_{1_i} | x_{2_j}) & 1 & 1 \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Probabilidades *a priori* de los lados de la moneda**: $p_{X_2}(0)= \\frac{11}{24} \\quad p_{X_2}(1)= \\frac{13}{24}$. \n",
    "\n",
    "**Probabilidades totales de las observaciones del dado**: $p_{X_1}(x_{1_i}) = \\sum_j p_{X_1 | X_2}(x_{1_i} | x_{2_j})p_{X_2}(x_{2_j})$\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c}\n",
    "x_{1_i}& p_{X_1}(x_{1_i}) \\\\\n",
    "\\hline\n",
    "1 & \\frac{1}{11}\\frac{11}{24}+ 0\\frac{13}{24}=\\frac{1}{24}\\\\\n",
    "\\hline\n",
    "2 & \\frac{2}{11}\\frac{11}{24} + \\frac{3}{13}\\frac{13}{24}=\\frac{5}{24}\\\\\n",
    "\\hline\n",
    "3 & \\frac{1}{11}\\frac{11}{24} + \\frac{4}{13}\\frac{13}{24}=\\frac{5}{24}\\\\\n",
    "\\hline\n",
    "4 & \\frac{2}{11}\\frac{11}{24} + \\frac{1}{13}\\frac{3}{24}=\\frac{3}{24}\\\\\n",
    "\\hline\n",
    "5 & \\frac{1}{11}\\frac{11}{24} + \\frac{3}{13}\\frac{4}{24}=\\frac{4}{24}\\\\\n",
    "\\hline\n",
    "6 & \\frac{4}{11}\\frac{11}{24} + \\frac{2}{13}\\frac{6}{24}=\\frac{6}{24}\\\\\n",
    "\\hline\n",
    "\\sum_i p_{X_1}(x_{1_i}) & 1 \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Probabilidades *a posteriori* de los lados de la moneda, dadas las observaciones del dado**: De nuevo seleccionamos cara (0) si sale 1, 4 ó 6 y cruz (1) en los restantes casos. Aunque en este ejemplo coinciden las elecciones utilizando los criterios **ML** (máxima verosimilitud) y **MAP** (máximo a posteriori), en general no tiene por qué ser así:\n",
    "\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c|c}\n",
    "p_{X_2 | X_1}(x_{2_j} | x_{1_i}) & 0 & 1 & \\sum_j p_{X_1X_2}(x_{1_i}x_{2_j})\\\\\n",
    "\\hline\n",
    "1 & 1 & 0 & 1\\\\\n",
    "\\hline\n",
    "2 & \\frac{2}{5} & \\frac{3}{5} & 1\\\\\n",
    "\\hline\n",
    "3 & \\frac{1}{5} & \\frac{4}{5} & 1\\\\\n",
    "\\hline\n",
    "4 & \\frac{2}{3} & \\frac{1}{3} & 1\\\\\n",
    "\\hline\n",
    "5 & \\frac{1}{4} & \\frac{3}{4} & 1\\\\\n",
    "\\hline\n",
    "6 & \\frac{4}{6} & \\frac{2}{6} & 1\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Probabilidades de acierto y de error**: para cada observación, el criterio **MAP** da la probabilidad de acertar, que es máxima. Por tanto, su complementario a 1 da la probabilidad de error, que es mínima:\n",
    "\n",
    "$$\n",
    "Acierto \\begin{array}{c|cccccc|c}\n",
    "  x_{1_i} & 1 & 2 & 3 & 4 & 5 & 6 &  \\\\ \n",
    "  \\hline\n",
    "    P(A | x_{1_i}) & 1 & \\frac{3}{5} & \\frac{4}{5} & \\frac{2}{3} & \\frac{3}{4} & \\frac{4}{6} & \\sum_i p_X(x_i)=1\n",
    " \\end{array}\n",
    "$$\n",
    "\n",
    "La probabilidad total de acierto es:\n",
    "\n",
    "$$P(A) = \\sum_i P(A | x_{1_i}) p_{x_{1_i}} =  \n",
    "1\\frac{1}{24} + \\frac{3}{5}\\frac{5}{24} + \\frac{4}{5}\\frac{5}{24} + \\frac{2}{3}\\frac{3}{24} + \\frac{3}{4}\\frac{4}{24} + \\frac{4}{6}\\frac{6}{24} =\\frac{17}{24}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "Error \\begin{array}{c|cccccc|c}\n",
    "  x_{1_i} & 1 & 2 & 3 & 4 & 5 & 6 &  \\\\ \n",
    "  \\hline\n",
    "    P(E | x_{1_i}) & 0 & \\frac{2}{5} & \\frac{1}{5} & \\frac{1}{3} & \\frac{1}{4} & \\frac{2}{6} & \\sum_i p_X(x_i)=1\n",
    " \\end{array}\n",
    "$$\n",
    "\n",
    "La probabilidad total de error es:\n",
    "\n",
    "$$P(E) = \\sum_i P(E | x_{1_i}) p_{x_{1_i}} =  \n",
    "0\\frac{1}{24} + \\frac{2}{5}\\frac{5}{24} + \\frac{1}{5}\\frac{5}{24} + \\frac{1}{3}\\frac{3}{24} + \\frac{1}{4}\\frac{4}{24} + \\frac{2}{6}\\frac{6}{24} =\\frac{7}{24}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La probabilidad conjunta nos proporciona toda la información precedente muy fácilmente:\n",
    "\n",
    "* Regla de decisión (ver tabla): (1, cara), (4, cara), (6, cara) (2, cruz), (3, cruz), (5, cruz)\n",
    "* $P(A) = p_{X_1X_2}(1,0)+p_{X_1X_2}(4,0)+p_{X_1X_2}(6,0)+p_{X_1X_2}(2,1)+p_{X_1X_2}(3,1)+p_{X_1X_2}(5,x)=\\frac{17}{24}$:\n",
    "\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c|c}\n",
    "p_{X_1X_2}(x_{1_i}, x_{2_j}) & 0 & 1 & p_{X_1}(x_{1_i}) = \\sum_j p_{X_1X_2}(x_{1_i}x_{2_j})\\\\\n",
    "\\hline\n",
    "1 & \\frac{1}{24} & 0 & \\frac{1}{24}\\\\\n",
    "\\hline\n",
    "2 & \\frac{2}{24} & \\frac{3}{24} & \\frac{5}{24}\\\\\n",
    "\\hline\n",
    "3 & \\frac{1}{24} & \\frac{4}{24} & \\frac{5}{24}\\\\\n",
    "\\hline\n",
    "4 & \\frac{2}{24} & \\frac{1}{24} & \\frac{3}{24}\\\\\n",
    "\\hline\n",
    "5 & \\frac{1}{24} & \\frac{3}{24} & \\frac{4}{24}\\\\\n",
    "\\hline\n",
    "6 & \\frac{4}{24} & \\frac{2}{24} & \\frac{6}{24}\\\\\n",
    "\\hline\n",
    "p_{X_2}(x_{2_j}) = \\sum_i p_{X_1X_2}(x_{1_i}x_{2_j}) & \\frac{11}{24} & \\frac{13}{24} & 1\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representación matricial\n",
    "\n",
    "El manejo de las tablas anteriores puede facilitarse haciendo uso de vectores y matrices de probabilidades. Considerando que la variable aleatoria $X$ puede tomar $M$ valores y que la $Y$ puede tomar $N$: \n",
    "\n",
    "* Vectores fila de probabilidades: \n",
    "  * $\\pi_X = [p_X(x_1), p_X(x_2) \\ldots p_X(x_M)]$\n",
    "  * $\\pi_Y = [p_Y(y_1), p_Y(y_2) \\ldots p_Y(y_N)]$\n",
    "* Matrices de probabilidades condicionadas. Cada fila corresponde a un valor de la variable condicionante y, por tanto, todas y cada una de las filas suman 1:\n",
    "  * $ \\Pi_{X | Y} = \\begin{bmatrix}\n",
    "  p_{X | Y}(x_1 | y_1) & \\ldots & p_{X | Y}(x_M | y_1)\\\\\n",
    "  \\vdots & \\ddots & \\vdots\\\\\n",
    "  p_{X | Y}(x_1 | y_N) & \\ldots & p_{X | Y}(x_M | y_N) & \n",
    " \\end{bmatrix}$\n",
    "  * $ \\Pi_{Y | X} = \\begin{bmatrix}\n",
    "  p_{Y | X}(y_1 | x_1) & \\ldots & p_{Y | X}(y_M | x_1)\\\\\n",
    "  \\vdots & \\ddots & \\vdots\\\\\n",
    "  p_{Y | X}(y_1 | x_N) & \\ldots & p_{Y | X}(y_M | x_N) & \n",
    " \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Teorema de la Probabilidad Total: se hace uso del producto de matrices para multiplicar un vector fila por una matriz de probabilidades condicionadas y obtener otro vector fila:\n",
    "  * $\\pi_Y = \\pi_X\\Pi_{Y | X}$\n",
    "  * $\\pi_X = \\pi_Y\\Pi_{X | Y}$\n",
    "* Matriz de probabilidades conjuntas. Sus elementos suman 1\n",
    "  * $ \\Pi_{XY} = \\begin{bmatrix}\n",
    "  p_{XY}(x_1,y_1) & \\ldots & p_{XY}(x_M,y_1)\\\\\n",
    "  \\vdots & \\ddots & \\vdots\\\\\n",
    "  p_{XY}(x_1,y_N) & \\ldots & p_{XY}(x_M,y_N) & \n",
    " \\end{bmatrix}$\n",
    " \n",
    "Recurriendo a operaciones puntuales (elemento a elemento) es posible obtener algoritmos sencillos para aplicar el Teorema de Bayes o para obtrener las matrices de probabilidades conjuntas a partir de las condicionadas y los vectores de probabilidades a priori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Función de dos variables aleatorias\n",
    "\n",
    "Consideremos un vector aleatorio discreto $(X,Y)$. Podemos aplicar sobre sus componentes una función $g$ que devolverá valores aleatorios, al serlo también sus variables independientes. Lo representamos como sigue:\n",
    "\n",
    "$$Z = g(X,Y)$$\n",
    "\n",
    "Cada par $(x_i, y_j)$ de $(X, Y)$ se transformará por la función en un nuevo valor $z_k$ de $Z$. Adviértase que valores distintos $(x_i,y_j) \\neq (x_m, y_n)$ pueden dar lugar a valores iguales $z_k = y_j$ con tal que $g(x_i,y_j)=g(x_m,y_n)$. Por tanto, definimos la función de masa de probabilidad de $Z$ como sigue:\n",
    "\n",
    "$$p_Z(z_k) = \\sum_{\\substack{i,j \\\\ g(x_i.y_j) = z_k}}p_{XY}(x_i, y_j)$$\n",
    "\n",
    "Adviértase que tal y como está expuesto no podemos invertir la trasformación, pues al tratarse de una función de dos variables necesitaríamos una segunda función. No vamos a profundizar en la formulación matemática, y vamos a desarrollar la intuición con un ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Supongamos el lanzamiento de sucesivo e independiente de dos monedas, cutos resultados modelamos con las variables aleatorias $X$ e $y$. En cada caso, cero significa cara y uno significa cruz. La funciones de masa de probabilidad marginales son las siguientes:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c}\n",
    "X & 0 & 1 \\\\\n",
    "\\hline\n",
    "p_X(x_i) & \\frac{1}{3} & \\frac{2}{3}  & \\sum_i p_X(x_i) = 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c}\n",
    "Y & 0 & 1 \\\\\n",
    "\\hline\n",
    "p_Y(x_j) & \\frac{5}{6} & \\frac{1}{6}  & \\sum_i p_Y(y_j) = 1\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Al ser ambas variaples aleatorias independientes puede obtenerse la distribución conjunta a partir de las marginales: $p_{XY}(x_i,y_j)=p_X(x_i)p_Y(y_j)$\n",
    "\n",
    "$$\n",
    "Y \\\\X \\begin{array}{c|c|c|c}\n",
    "p_{XY}(x_{i}, y_{j}) & 0 & 1 & p_{X}(x_{i}) = \\sum_j p_{XY}(x_{i}y_{j})\\\\\n",
    "\\hline\n",
    "0 & \\frac{1}{3}\\frac{5}{6} = \\frac{5}{18} & \\frac{1}{3}\\frac{1}{6} = \\frac{1}{18} & \\frac{1}{3}\\\\\n",
    "\\hline\n",
    "1 & \\frac{2}{3}\\frac{5}{6} = \\frac{10}{18} & \\frac{2}{3}\\frac{1}{6} = \\frac{2}{18} & \\frac{2}{3}\\\\\n",
    "\\hline\n",
    "p_{Y}(y_{j}) = \\sum_i p_{XY}(x_{i}y_{j}) & \\frac{5}{6} & \\frac{1}{6} & 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "La función de masa de probabilidad de la suma de ambas variables aleatorias es:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c}\n",
    "Z = X+Y & 0 & 1 & 2\\\\\n",
    "\\hline\n",
    "p_Z(z_k) & \\frac{5}{8} & \\frac{1}{18} +\\frac{10}{18} = \\frac{11}{18} &\\frac{2}{18}  & \\sum_i p_Y(y_j) = 1\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una propiedad muy importante de la **suma de dos variables aleatorias discretas independientes** permite obtener la función de masa de probabilidad de la variable aleatoria suma a partir de las de las dos sumandos:\n",
    "\n",
    "> La función de masa de probabilidad de la suma de dos variables aleatorias discretas **independientes**, $Z = X+Y$, se obtiene mediante la **convolución** discreta de las funciones de masa de probabilidad de de cada uno de los sumandos:\n",
    "\n",
    "$$p_Z(z_i)= p_X(x_i)*p_Y(y_i)=\\sum_{k=-\\infty}^\\infty p_X(x_k)p_Y(y_{i-k})=\\sum_{k=-\\infty}^\\infty p_Y(y_k)p_X(x_{i-k}) $$\n",
    "\n",
    "La convolución es una operación matemática que se realiza entre dos secuencias o señales y que se verá en la asignatura de Señales y Sistemas. No vamos a profundizar aquí en su cálculo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Esperanza Matemática y Valor Medio. Momentos\n",
    "\n",
    "\n",
    "Se define la esperanza matemática de una función $g(X, Y)$ de dos variables aleatorias discretas como sigue:\n",
    "\n",
    "$$E\\left(g(X, Y)\\right)=\\sum_i\\sum_j g(x_i,y_j)p_{XY}(x_i,y_j)$$\n",
    "\n",
    "Si las variables aleatorias son continuas:\n",
    "\n",
    "$$E\\left(g(X, Y)\\right)=\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty g(x,y)f_{XY}(x,y)dxdy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Como ya se vio en el caso unidimensional, es fácil advertir que el operador $E(·)$ es **lineal**:\n",
    "\n",
    "$$E(a_1g_1(X,Y)+a_2g_2(X,Y))=a_1E(g_1(X,Y)) + a_2E(g_2(X,Y))$$\n",
    "\n",
    "donde $a_1$ y $a_2$ son coeficiente constantes arbitrarios y $g_1$ y $g_2$ son dos funciones. También es evidente que, si $k$ es una constante arbitraria\n",
    "\n",
    "$$E(k) = k$$\n",
    "\n",
    "De la linealidad se concluye inmediatamente:\n",
    "\n",
    "$$E(X+Y) = E(X) + E(Y)$$\n",
    "\n",
    "$$E(kX) = kE(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Veamos como obtener las esperanzas marginales en el caso discreto. En el caso continuo sería igual, sin más que cambiar funciones de masa de probabilidad por densidades y sumas por integrales. \n",
    "\n",
    "Si $g(X,Y)=X$ se obtiene el valor medio de $X$:\n",
    "\n",
    "$$\\eta_X=E(X)=\\sum_i\\sum_j x_i p_{XY}(x_i,y_j)=\\sum_i x_i \\sum_j p_{XY}(x_i,y_j) = \\sum_i x_i p_X(x_i)$$\n",
    "\n",
    "Y si $g(X,Y)=Y$ se obtiene el valor medio de $Y$:\n",
    "\n",
    "$$\\eta_Y=E(Y)=\\sum_i\\sum_j x_i p_{XY}(x_i,y_j)=\\sum_j y_j \\sum_i p_{XY}(x_i,y_j) = \\sum_j y_j p_Y(y_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Valor Cuadrático Medio y Correlación\n",
    "\n",
    "Considérese ahora que $g(X,Y)=(X+Y)^2$, esto es, calculamos el valor cuadrático medio de la suma de $X$ e $Y$. Desarrollando el cuadrado y aplicando linealidad:\n",
    "\n",
    "$$E((X+Y)^2)= E(X^2)+E(Y^2)+2E(XY)$$\n",
    "\n",
    "El término $R_{XY}=E(XY)$ se denomina **correlación**, y es un indicador de la dependencia probabilística de ambas variales aleatorias, $X$ e $Y$.\n",
    "\n",
    "* $X$ e $Y$ son **ortogonales** si $R_{XY}=E(XY)=0$. Ello indica que no podemos pensar que ambas variables guarden una relación de proporcionalidad $Y \\not \\approx kX$ pues ello supondría que $E(X^2) \\approx 0$.\n",
    "* En general, $E(XY) \\neq E(X)E(Y)$. Como veremos, si $E(XY)=E(X)E(Y)$ ambas variables aleatorias están **incorreladas**.\n",
    "* Si $X$ e $Y$ son independientes es fácil advertir que $E(XY)=E(X)E(Y)$, de modo que <u>la independencia implica incorrelación</u>, si bien el contrario no es cierto en general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Es habitual utilizar una formulación matricial, haciendo uso de la **matriz de correlación**:\n",
    "\n",
    "$$\\mathbf R_{XY} = E(\\begin{pmatrix} X\\\\Y\\end{pmatrix} (X, Y))= \n",
    "\\begin{pmatrix}\n",
    "E(X^2) & E(XY)\\\\\n",
    "E(XY) & E(Y^2)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Como se ve, la matriz de correlación es una matriz simétrica, cuyos elementos sobre la diagonal corresponden a los valores cuadrático medio, $E(X^2)$ y $E(Y^2)$ y fuera de la misma está la correlación entre $X$ e $Y$, $E(XY)$. \n",
    "\n",
    "Si ambas variables fueran ortogonales, la matriz de correlación sería diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Covarianza\n",
    "\n",
    "Considérese ahora que centramos el vector aleatorio $(X,Y)$ en el centro de masa probabilístico $(\\eta_X, \\eta_Y)$ y que $g(X,Y)=\\left( (X-\\eta_X)+(Y-\\eta_Y) \\right)^2$\n",
    "\n",
    "$$E\\left(((X-\\eta_X)+(Y-\\eta_Y))^2\\right)= \\sigma_X^2+\\sigma_Y^2+2E((X-\\eta_X)(Y-\\eta_Y)$$\n",
    "\n",
    "El término $C_{XY}=E((X-\\eta_X)(Y-\\eta_Y))$ se denomina **covarianza**, y es también un indicador de la dependencia probabilística de ambas variales aleatorias, $X$ e $Y$.\n",
    "\n",
    "* La covarianza se relaciona con la correlación sin más que operar y aplicar linealidad: $C_{XY}=R_{XY}-\\eta_X\\eta_Y$\n",
    "* $X$ e $Y$ son incorreladas si $C_{XY}=0$. Decir que $X$ e $Y$ están incorreladas es equivalente a afirmar que $X-\\eta_X$ e $Y-\\eta_Y$ son ortogonales.\n",
    "* Es fácil demostrar que $C_{XY}= 0 \\iff R_{XY}=\\eta_X\\eta_Y$.\n",
    "* La <u>incorrelación supone ausencia de relación lineal</u> entre $X$ e $Y$, esto es, $Y \\not \\approx aX + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Es habitual utilizar una formulación matricial, haciendo uso de la **matriz de covarianza**:\n",
    "\n",
    "$$\\mathbf{C_{XY}} = E\\left(\\begin{pmatrix} X-\\eta_X\\\\Y-\\eta_Y\\end{pmatrix} (X-\\eta_X, Y-\\eta_Y)\\right)=\\\\\n",
    "\\begin{pmatrix}\n",
    "E((X-\\eta_X)^2) & E((X-\\eta_X)(Y-\\eta_Y))\\\\\n",
    "E((X-\\eta_X)(Y-\\eta_Y)) & E((Y-\\eta_Y)^2)\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\sigma_X^2 & C_{XY}\\\\\n",
    "C_{XY} & \\sigma_Y^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Como se ve, la matriz de covarianza es una matriz simétrica, cuyos elementos sobre la diagonal corresponden a las varianzas, $\\sigma_X^2$ y $\\sigma_Y^2$ y fuera de la misma está la covarianza entre $X$ e $Y$, $C_{XY}$. \n",
    "\n",
    "Si ambas variables fueran incorreladas, la matriz de covarianza sería diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Coeficiente de correlación\n",
    "\n",
    "Si ahora trabajamos con las variables aletorias tipificadas, $\\hat X = \\frac{X-\\eta_X}{\\sigma_X}$ e $\\hat Y = \\frac{Y-\\eta_Y}{\\sigma_Y}$ podemos definir el **coeficiente de correlación** (recuérdese que las variables tipificadas tienen, por construcción, valor medio nulo y desviación típica unidad):\n",
    "\n",
    "$$r_{XY}=E(\\frac{X-\\eta_X}{\\sigma_X} \\frac{Y-\\eta_Y}{\\sigma_Y})=\\frac{1}{\\sigma_X\\sigma_Y}C_{XY}  \\qquad -1 \\leq r_{XY} \\leq 1$$\n",
    "\n",
    "El coeficiente de correlación $r_{XY}$:\n",
    "\n",
    "* Tiene valor nulo si hay ausencia de relación lineal entre $X$ e $Y$\n",
    "* Tiene valor 1 si $Y = aX +b$ con $a>0$\n",
    "* Tiene valor -1 si  $Y = aX +b$ con $a<0$\n",
    "* Si $0<|r_{XY}|<1$ el grado de dependencia lineal (positiva o negativa) será tanto mayor cuanto más cerca de 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Esperanza matemática condicionada\n",
    "\n",
    "Podemos definir la esperanza matemática de una función $g$ de las variables aleatorias $(X,Y)$ condicionada por el suceso $M$ como sigue, si son discretas:\n",
    "\n",
    "$$E(g(X,Y) | M)=\\sum_i g(x_i,y_j)p_{XY}(x_i,y_j | M)$$\n",
    "\n",
    "Si las variables aleatorias son continuas:\n",
    "\n",
    "$$E(g(X,Y) | M)=\\int_{-\\infty}^\\infty g(x,y)f_{XY}(x,y | M)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos extender con la esperanza condicionada las propiedades y definiciones vistas anteriormente. Lo ilustraremos para el caso de variables aleatorias discretas, si bien es igual para continuas, sin más que utilizar funciones de densidad e integrales en vez de funciones de masa de probabilidad y sumatorios. Por ejemplo:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\eta_{X | M}=E(X | M)&=\\sum_i \\sum_j x_ip_{XY}(x_i,y_j | M)= \\sum_i x_i p_X(x_i | M)\\\\\n",
    "E(X^2 | M)&=\\sum_i \\sum_j x_i^2p_{XY}(x_i,y_j | M)=\\sum_i x_i^2 p_X(x_i | M)\\\\\n",
    "\\sigma_{X | M}^2 = Var(X | M) &= E(X^2 | M)-\\eta_{X | M}^2\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En particular, podemos considerar los momentos de $X$ condicionados por $Y=y_j$, donde utilizamos el abuso de notación $E(\\bullet \\ | \\ \\{Y=y_j\\}) \\equiv E(\\bullet \\ | y_j)$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\eta_{X/+ | y_j}=E(X | y_j)&= \\sum_i x_i p_{X | Y}(x_i | y_j)\\\\\n",
    "E(X^2 | y_j)&=\\sum_i x_i^2 p_X(x_i | y_j)\\\\\n",
    "\\sigma_{X | y_j}^2 = Var(X | y_j) &= E(X^2 | y_j)-\\eta_{X | y_j}^2\n",
    "\\end{align*}$$\n",
    "\n",
    "Adviértase que $E(X) = \\sum_j E(X | y_j) p_Y(y_j)$, sin más que sustituir $E(X | y_j)$ por su definición y utilizar el Teorema de la Probabilidad Total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Consideramos el ejemplo ya visto de lanzamiento mutuamente dependiente de un dado y una moneda trucados, cuya función de masa de probabilidad se indica de nuevo:\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c|c}\n",
    "p_{X_1X_2}(x_{1_i}, x_{2_j}) & 0 & 1 & p_{X_1}(x_{1_i}) = \\sum_j p_{X_1X_2}(x_{1_i}x_{2_j})\\\\\n",
    "\\hline\n",
    "1 & \\frac{1}{24} & 0 & \\frac{1}{24}\\\\\n",
    "\\hline\n",
    "2 & \\frac{2}{24} & \\frac{3}{24} & \\frac{5}{24}\\\\\n",
    "\\hline\n",
    "3 & \\frac{1}{24} & \\frac{4}{24} & \\frac{5}{24}\\\\\n",
    "\\hline\n",
    "4 & \\frac{2}{24} & \\frac{1}{24} & \\frac{3}{24}\\\\\n",
    "\\hline\n",
    "5 & \\frac{1}{24} & \\frac{3}{24} & \\frac{4}{24}\\\\\n",
    "\\hline\n",
    "6 & \\frac{4}{24} & \\frac{2}{24} & \\frac{6}{24}\\\\\n",
    "\\hline\n",
    "p_{X_2}(x_{2_j}) = \\sum_i p_{X_1X_2}(x_{1_i}x_{2_j}) & \\frac{11}{24} & \\frac{13}{24} & 1\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Variable aleatoria $X_1$ (dado)\n",
    "  * $\\eta_{X_1}=E(X_1)=1\\frac{1}{24}+2\\frac{5}{24}+3\\frac{5}{24}+4\\frac{3}{24}+5\\frac{4}{24}+6\\frac{6}{24}=\\frac{94}{24}\\approx3.917$\n",
    "  * $E(X_1^2)=1^2\\frac{1}{24}+2^2\\frac{5}{24}+3^2\\frac{5}{24}+4^2\\frac{3}{24}+5^2\\frac{4}{24}+6^2\\frac{6}{24}=\\frac{430}{24}\\approx 17.917$\n",
    "  * $Var(X_1)=E(X_1^2) -\\eta_X^2= \\frac{430}{24}-(\\frac{94}{24})^2 \\approx 2.576$\n",
    "  * $\\sigma_{X_1} \\approx 1.605$\n",
    "* Variable aleatoria $X_2$ (moneda)\n",
    "  * $\\eta_{X_2}=E(X_2)= 0\\frac{11}{24}+1\\frac{13}{24}=\\frac{13}{24}\\approx 0.542$\n",
    "  * $E(X_2^2)= 0^2\\frac{11}{24}+1^2\\frac{13}{24}=\\frac{13}{24}\\approx 0.542$\n",
    "  * $Var(X_2)=E(X_2^2) -\\eta_{X_2}^2= \\frac{13}{24}-(\\frac{13}{24})^2=\\frac{13}{24}\\frac{11}{24} \\approx 0.248$\n",
    "  * $\\sigma_{X_2} \\approx 0.498$\n",
    "  * Momentos conjuntos \n",
    "    * Correlación: $R_{XY} = 2\\frac{3}{24}+3\\frac{4}{24}+4\\frac{1}{24}+5\\frac{3}{24}+6\\frac{2}{24}=\\frac{49}{24} \\approx 2.042$\n",
    "    * Covarianza: $C_{XY}=R_{XY}-\\eta_X\\eta_Y= \\frac{49}{24}-\\frac{94}{24}\\frac{13}{24}\\approx -0.80$\n",
    "    * Coeficiente de correlación: $r_{XY} = \\frac{1}{\\sigma_X\\sigma_Y}C_{XY} \\approx -0.1 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Con $p_{X_1 | X_2}(x_1, x_2)$ pueden calcularse los momentos de los resultados del dado para cada resultado del lanzamiento de la moneda. Por ejemplo, la media:\n",
    "\n",
    "$$\n",
    "X_2 \\\\X_1 \\begin{array}{c|c|c}\n",
    "p_{X_1 | X_2}(x_{1_i} | x_{2_j}) & 0 & 1\\\\\n",
    "\\hline\n",
    "1 & \\frac{1}{11} & 0\\\\\n",
    "\\hline\n",
    "2 & \\frac{2}{11} & \\frac{3}{13}\\\\\n",
    "\\hline\n",
    "3 & \\frac{1}{11} & \\frac{4}{13}\\\\\n",
    "\\hline\n",
    "4 & \\frac{2}{11} & \\frac{1}{13}\\\\\n",
    "\\hline\n",
    "5 & \\frac{1}{11} & \\frac{3}{13}\\\\\n",
    "\\hline\n",
    "6 & \\frac{4}{11} & \\frac{2}{13}\\\\\n",
    "\\hline\n",
    "E(X_1 | x_{2_j}) & \n",
    "1\\frac{1}{11}+2\\frac{2}{11}+3\\frac{1}{11}+4\\frac{2}{11}+5\\frac{1}{11}+6\\frac{4}{11}=\\frac{45}{11}\n",
    "& 0+2\\frac{3}{13}+3\\frac{4}{13}+4\\frac{1}{13}+5\\frac{3}{13}+6\\frac{2}{13}=\\frac{49}{13}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Por supuesto, $E(X_1)=E(X_1/0)p_{X_2}(0)+E(X_1/1)p_{X_2}(1) = \\frac{45}{11}\\frac{11}{24}+\\frac{49}{13}\\frac{13}{24}=\\frac{94}{24}$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
