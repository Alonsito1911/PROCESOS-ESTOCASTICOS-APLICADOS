{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II.3 Variable Aleatoria Continua\n",
    "\n",
    "Recordemos que, dado el **espacio de probabilidad** $\\mathscr{E}(\\Omega, \\mathscr{F}, P)$, definimos la **variable aleatoria** $X$\n",
    "\n",
    "$$\\begin{align*}\n",
    "X:  \\mathscr{\\Omega} & \\longmapsto \\mathbb{R}\\\\\n",
    "   \\alpha & \\longmapsto X(\\alpha)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "Consideremos ahora que la variable aleatoria $X$ toma valores sobre el conjunto de los números reales $\\mathbb{R}$, *variando de forma continua la asignación de probabilidades en intervalos de la recta real*. Esto hace que, en general, las probabilidades asignadas a cada valor (pero no a cada intervalo) de la variable aleatoria sean nulas:\n",
    "$$P(X=x)=0 \\quad \\forall x \\in \\mathbb{R}$$\n",
    "\n",
    "Sin perjuicio de que, si hay algunos $x_i \\in \\mathbb{R}$ con masas de probabilidad asignadas y $\\sum_i P(x_i) < 1$, de modo que parte de a probabilidad se distribuye de forma continua en intervalos, nos encontremos ante una **variable aleatoria mixta**, por compartir características de las discretas y de las continuas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Función de distribución de probabilidad\n",
    "\n",
    "Puede demostrarse que cualquier suceso de $\\mathscr{F}$ puede obtenerse mediante operaciones conjutistas realizadas sobre sucesos definidos sobre semi-intervalos de la forma $\\{X\\leq x\\}$ donde $x \\in \\mathbb{R}$ es un nñumero real arbitrario. Si se tienen las probabilidades de tales sucesos se conoce totalmente el espacio de probabilidad de la variable aleatoria, lo que justifica la definición de **función de distribución de probabilidad** como:\n",
    "\n",
    "$$F_X(x) = P(\\{X\\leq x\\}) \\quad -\\infty \\leq x \\leq \\infty$$\n",
    "\n",
    "El **cuantil** $x_q$ corresponde al valor que hace que el $q\\%$ de la distribución esté por debajo. Por tanto, $q = F_X(x_q) \\implies x_q = F_X^{-1}(q)$ y $F_X^{-1}$ se llama **función cuantil**. Por ejemplo,\n",
    "* El primer cuartil es $x_{0.25} = F_X^{-1}(0.25)$\n",
    "* La mediana o segundo cuartil es $x_{0.5} = F_X^{-1}(0.5)$\n",
    "* El tercer cuartil es $x_{0.75} = F_X^{-1}(0.75)$\n",
    "* El octavo decil es $x_{0.8} = F_X^{-1}(0.8)$\n",
    "* El décimoquinto percentil es $x_{0.15} = F_X^{-1}(0.15)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Propiedades de la función de distribución de probabilidad\n",
    "\n",
    "Sean $F_X(x^-)=\\lim \\limits_{\\epsilon \\to 0} F_X(x-\\epsilon)$ y $F_X(x^+)=\\lim \\limits_{\\epsilon \\to 0} F_X(x+\\epsilon)$ los valores de la función de distribución en $x$ al acercarse respectivamente por la izquierda y por la derecha. Se cumplen las siguientes propiedades:\n",
    "\n",
    "1. $F_X(-\\infty) = 0 \\quad F_X(+\\infty) = 1$\n",
    "2. $P(\\{X>x\\})=1-F_X(x)$\n",
    "3. $F_X$ es continua por la derecha: $F_X(x^+)=F_X(x)$\n",
    "4. Si la variable aleatoria es continua, $F_X$ también lo es y $F_X(x^-)=F_X(x)$\n",
    "4. $F_X$ es no decreciente: $x_1 < x_2 \\implies F_X(x_1)\\leq F_X(x_2)$\n",
    "5. $F_X(x_0)=0 \\implies F_X(x) = \\forall x \\leq x_0$\n",
    "6. En los puntos de discontinuidad, $P(\\{X = x\\})=F_X(x)-F_X(x^-)$\n",
    "7. $P(\\{x_1<X\\leq x_2\\})=F_X(x_2)-F_X(x_1)$\n",
    "8. $P(\\{x_1 \\leq X\\leq x_2\\})=F_X(x_2)-F_X(x_1^-)$\n",
    "9. Si la **variable aleatoria es discreta**, la función de distribución es constante salvo en los puntos de discontinuidad en los que se sitúan los sucesos muestrales elementales.\n",
    "10. La **variable aleatoria puede ser mixta**, con sucesos muestrales con probabilidad finita no nula y otros definidos sobre un continuo con probabilidades elementales nulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Función de densidad de probabilidad\n",
    "\n",
    "Suele ser conveniente hacer un símil entre probabilidad y masa, de modo que nuestra distribución de probabilidad a lo largo de probabilidad podemos interpretarla como una masa igualmente distribuida a lo largo del eje.\n",
    "\n",
    "Si la **variable es discreta**, tendremos **masas puntuales** ideales mientras que si la **variable es continua**, la masa se distribuirá concentrándose más o menos a lo largo del eje, pero sin llegar a formar masas puntuales. La **variable mixta** combina ambos tipos de masa a lo largo del eje.\n",
    "\n",
    "Como ya se ha visto, **la función de distribución describe la masa acumulada hasta una posición** y, por diferencias, puede calcularse la masa de cualquier intervalo.\n",
    "\n",
    "La **función de densidad**, como su nombre indica, describe la densidad de masa (masa por unidad de longitud) a lo largo del eje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La **función de densidad de probabilidad** (fdp) se define:\n",
    "\n",
    "$$f_X(x) =\\frac{dF_X(x)}{dx}=\\lim \\limits_{\\Delta x \\to 0} \\frac{F_X(x+\\Delta x/2)-F_X(x-\\Delta x/2)}{\\Delta x}= \n",
    "\\lim \\limits_{\\Delta x \\to 0} \\frac{P\\left(\\{(x-\\Delta x/2) < x \\leq (x+\\Delta x/2)\\}\\right)}{\\Delta x}\n",
    "$$\n",
    "\n",
    "Si **la variable aleatoria es continua**, \n",
    "$$P\\left(\\{(x-\\Delta x/2) < x \\leq (x+\\Delta x/2)\\}\\right) \\approx f_X(x)\\Delta x$$\n",
    "\n",
    "Si **la variable aleatoria es discreta**, $F_X(x)$ resulta discontinua por la izquierda en los puntos donde se localizan masas de probabilidad y no es derivable en los mismos. En tales puntos ${x_i}$ puede definirse una **función de masa de probabilidad** $p_X(x_i)=P(\\{X=x_i\\})$.\n",
    "\n",
    "Matemáticamente, puede usuarse la función $\\delta(x)$, que tiene **área uno** localizada en $x=0$, anchura nula y altura infinita para relacionar las funciones de masa y de densidad de probabilidad. Si la variable aleatoria es discreta:\n",
    "$$f_X(x)=\\sum_{i}p_X(x_i)\\delta(x-x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Propiedades de la función de densidad de probabilidad (fdp)\n",
    "\n",
    "La función de distribución de probabilidad (fdp) se obtiene integrando:\n",
    "\n",
    "$$f_X(x) = \\frac{dF_X(x)}{dx} \\iff F_X(x)=\\int_{-\\infty}^x f_X(u)du$$\n",
    "\n",
    "Dado que $F_X(\\infty)=1$ se concluye que $\\int_{-\\infty}^{\\infty} f_X(x)dx = 1$. \n",
    "\n",
    "Por tanto, el área debajo de la función de densidad de probabilidad **siempre** es uno. Esto es lógico, pues el eje real corresponde al espacio muestral o suceso seguro.\n",
    "\n",
    "La probabilidad de un intervalo es\n",
    "$$P(x_1< X \\leq x_2) = F_X(x_2)-F_X(x_1)=\\int_{x_1}^{x_2}f_X(x)dx$$\n",
    "\n",
    "y corresponde al área debajo de la curva de la fdp en dicho intervalo. Adviértase que $\\int_{0^-}^{0^+}\\delta(x)dx = 1$, lo que nos permite extender el formalismo a variables discretas y mixtas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Propiedades de la función de masa de probabilidad (fmp)\n",
    "\n",
    "Como ya se ha visto, cuando la variable aleatoria es discreta, se define la **función de masa de probabilidad**, que se relaciona como sigue con la fdp:\n",
    "\n",
    "$$p_X(x_i)=P(\\{X=x_i\\})\\iff f_X(x)=\\sum_{i}p_X(x_i)\\delta(x-x_i)$$\n",
    "\n",
    "Podemos relacionar la función de masa de probabilidad (fmp) con la función de distribución de probabilidad (FDP):\n",
    "\n",
    "$$p_X(x_n) = F_X(x_n)-F_X(x_{n-1}) \\iff F_x(x_n)=\\sum_{i=-\\infty}^n p_X(x_i)$$\n",
    "\n",
    "Siguen fácilmente las siguientes propiedades:\n",
    "\n",
    "$$F_x(\\infty)=1 \\implies \\sum_{i=-\\infty}^{+\\infty} p_X(x_i)=1$$\n",
    "\n",
    "$$P(x_{n_1} \\leq X \\leq x_{n_2}) = \\sum_{i=n_1}^{n_2} p_X(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Relación entre el histograma y la fdp\n",
    "\n",
    "Supongamos que tenemos una muestra estadística con $N$ observaciones de una variable estadística. Podemos considerar que tales observaciones corresponden a realizaciones de un experimento modelado por un espacio de probabilidad sobre el que hemos definido una variable aleatoria correspondiente con la variable estadística.\n",
    "\n",
    "El espacio de probabilidad propociona un modelo teórico de los datos, mientras que las muestra estadísitca resulta de la experiencia empírica. ¿Podemos caracterizar probabilísticamente la población a partir de la muetsra estadística?\n",
    "\n",
    "Como sabemos podemos construir un histograma de frecuencias relativas, sin más que agrupar los valores de la variable estadística por intervalos. Considerando que todos los intervalos tienen igual anchura $\\Delta x$, podemos aproximar la fdp en cada $x_i$ (puntos medios del intervalo, como sigue, siendo $F_i$ la frecuencia absoluta de observaciones que caen dentro del intervalo:\n",
    "\n",
    "$$f_X(x_i) \\approx \\frac{F_i}{N\\Delta x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esperanza matemática y momentos\n",
    "\n",
    "La esperanza matemática o media de una variable aleatoria $X$ es:\n",
    "\n",
    "$$\\eta_X = E\\{X\\}=\\int_{-\\infty}^{\\infty}xf_X(x)dx$$\n",
    "\n",
    "Si consideramos la aproximación anterior del histograma a la fdp, podemos ver que la media muestral aproxima la esperanza matemática: $\\hat x = \\sum_{i=1}^Nx_i\\frac{F_i}{\\Delta x} \\approx \\eta_X$.\n",
    "\n",
    "La esperanza matemática puede definirse sobre cualquier función $g(X)$ de la variable aleatoria:\n",
    "\n",
    "$$\\eta_{g(X)} = E\\{g(X)\\}=\\int_{-\\infty}^{\\infty}g(x)f_X(x)dx$$\n",
    "\n",
    "La **esperanza matemática es un operador lineal**, esto es, sean $g_1(X)$ y $g_2(X)$:\n",
    "$$E\\{a_1g_1(X)+a_2g_2(X)\\}=a_1E\\{g_1(X)\\}+a_2E\\{g_2(X)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos así definir los **momentos**:\n",
    "\n",
    "$$m_n=E\\{X^n\\}=\\int_{-\\infty}^{\\infty}x^nf_X(x)dx$$\n",
    "\n",
    "En particular, la **media** es $m_1\\equiv\\eta_X=E\\{X^1\\}$ y el **valor cuadrático medio** es $m_2=E\\{X^2\\}$\n",
    "\n",
    "Y los **momentos centrales**:\n",
    "\n",
    "$$\\mu_n=E\\{(X-\\eta_X)^n\\}=\\int_{-\\infty}^{\\infty}(x-\\eta_X)^nf_X(x)dx$$\n",
    "\n",
    "En particular, la **varianza** es $m_2\\equiv \\sigma_X^2 =E\\{(X-\\eta_X)^2\\}$. Aplicando la linealidad:\n",
    "\n",
    "$$\\sigma_X^2=E\\{(X-\\eta_X)^2\\}=E\\{X^2\\}-\\eta_X^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Esperanza y momentos en el caso discreto\n",
    "\n",
    "Si la **variable aleatoria es discreta**, se utilizan las funciones de masa de probabilidad en vez de las fdp, y sumas en vez de integrales:\n",
    "\n",
    "$$\\eta_{g(X)} = E\\{g(X)\\}=\\sum_{i=-\\infty}^{\\infty}g(x_i)p_X(x_i)$$\n",
    "\n",
    "$$m_n=E\\{X^n\\}=\\sum_{i=-\\infty}^{\\infty}x^nf_X(x)dx$$\n",
    "\n",
    "$$\\mu_n=E\\{(X-\\eta_X)^n\\}=\\sum_{i=-\\infty}^{\\infty}(x-\\eta_X)^nf_X(x)dx$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Todos estos momentos tienen sus equivalentes muestrales, ya vistos en la parte de introducción a la estadística. \n",
    "\n",
    "Por otra parte, es muy habitual **normalizar** una variable aleatoria $X$, construyendo otra $Y=\\frac{X-\\eta_X}{\\sigma_X}$ que tenga media nula y varianza unidad.\n",
    "\n",
    "Además, podemos definir:\n",
    "\n",
    "* Coeficiente de asimetría: $\\gamma_1=E\\{\\frac{(X-\\eta_X)^3}{\\sigma^3}\\}=\\frac{\\mu_3}{\\sigma_X^3}$ \n",
    "* Coeficiente de exceso de curtosis: $\\gamma_2=E\\{\\frac{(X-\\eta_X)^4}{\\sigma^4}\\}-3=\\frac{\\mu_4}{\\sigma_X^4}-3$. La distribución normal tiene un exceso de curtosis de 0, razón por la que se resta 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funciones condicionadas\n",
    "\n",
    "Consideremos un espacio de probabilidad $\\mathscr{E}$ en el que se ha definido una variable aleatoria $X$, su función de distribución $F_X(x)$ y su función de densidad $f_X(x)$. Considérese, además, un suceso $A \\in \\mathscr{F}$.\n",
    "\n",
    "La **función de distribución de $X$ condicionada por $A$** es: \n",
    "\n",
    "$$F_X(x\\, /\\, A) = P(X \\leq x \\, /\\, A) = \\frac{P(\\{X \\leq x \\bigcap A )\\} }{P(A)}$$\n",
    "\n",
    "Y la **función de densidad de $X$ condicionada por $A$** es:\n",
    "\n",
    "$$f_X(x \\, /\\, A)=\\frac{dF_X(x \\, /\\, A)}{dx}=\\lim \\limits_{\\Delta x \\to 0} \\frac{F_X(x+\\Delta x/2 \\, /\\, A)-F_X(x-\\Delta x/2 \\, /\\, A)}{\\Delta x}= \\\\\n",
    "=\\lim \\limits_{\\Delta x \\to 0} \\frac{P\\left(\\{(x-\\Delta x/2) < x \\leq (x+\\Delta x/2)\\} \\, /\\, A \\right)}{\\Delta x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En general el suceso condicionante $A$ podemos definirlo con intervalos de la variable aleatoria. Suponiendo que $A=\\{x_1\\leq X \\leq x_2\\}$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "F_X(x\\, /\\, \\{x_1\\leq X \\leq x_2\\}) & = \\frac{P(\\{X \\leq x \\bigcap \\{x_1\\leq X \\leq x_2\\ )\\} }{P(\\{x_1\\leq X \\leq x_2\\})}=\\\\\n",
    " & = \\begin{cases}\n",
    "    0       & \\quad x < x_1\\\\\n",
    "    \\frac{F_X(x)-F_X(x_1^-)}{F_X(x_2)-F_X(x_1^-)}  & \\quad x_1 \\leq x \\leq x_2\\\\\n",
    "    1 &\\quad x > x_2\n",
    "  \\end{cases}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "f_X(x\\, /\\, \\{x_1\\leq X \\leq x_2\\}) = \\begin{cases}\n",
    "    0       & \\quad x < x_1\\\\\n",
    "    \\frac{f_X(x)}{F_X(x_2)-F_X(x_1^-)}  & \\quad x_1 \\leq x \\leq x_2\\\\\n",
    "    0 &\\quad x > x_2\n",
    "  \\end{cases}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Teorema de Bayes\n",
    "\n",
    "Recordemos el **Teorema de Bayes** con dos sucesos:\n",
    "\n",
    "$$P(A/B) = \\frac{P(B/A)P(A)}{P(B)}$$\n",
    "\n",
    "Si $B=\\{X \\leq x\\}$:\n",
    "\n",
    "$$P(A\\, / \\, X \\leq x) = \\frac{F_X(x \\, / \\, A)P(A)}{F_X(x)} \\iff F_X(x \\, / \\, A) = \\frac{P(A\\, / \\, X \\leq x)F_X(x)}{P(A)}$$\n",
    "\n",
    "Si $B=\\{x_1 < X \\leq x_2\\}$:\n",
    "\n",
    "$$P(A\\, / \\{x_1 < X \\leq x_2\\}) = \\frac{F_X(x_2 \\, / \\, A)-F_X(x_1 \\, / \\, A)}{F_X(x_2)-F_X(x_1)}P(A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si $B=\\{X=x\\}$ y la variable aleatoria es continua nos encontramos con un problema para condicionar por $B$ pues su probabilidad es nula. En tal caso consideramos que $B=\\{x<X-\\Delta x/2 \\leq x+\\Delta x/2\\}$ y tomamos un paso al límite:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(A \\, / \\, X=x) & = \\lim \\limits_{\\Delta x \\to 0} P(A\\, / \\{x-\\Delta x/2 < X \\leq x + \\Delta x/2\\}) \\\\\n",
    "& = \\lim \\limits_{\\Delta x \\to 0} \\frac{F_X(x +\\Delta x/2 \\, / \\, A)-F_X(x-\\Delta x/2 \\, / \\, A)}{F_X(x+\\Delta x/2)-F_X(x-\\Delta x/2)}P(A)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Por tanto,\n",
    "\n",
    "$$ P(A \\, / \\, X=x) = \\frac{f_X(x \\, / \\, A)P(A)}{f_X(x)} \\iff f_X(x \\, / \\, A) = \\frac{P(A \\, / \\, X=x)f_X(x)}{P(A)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Teorema de la Probabilidad Total\n",
    "\n",
    "Sea $A_i \\in \\mathscr{P}(\\Omega) \\, i=1\\ldots K$ es una partición del espacio muestral $\\Omega$. Se obtiene inmediatamente el **Teorema de la Probabilidad Total**\n",
    "\n",
    "$$F_X(x) = F_X(x \\, / \\, A_1)P(A_1)+\\ldots + F_X(x \\, / \\, A_K)P(A_K)$$\n",
    "\n",
    "Derivando:\n",
    "\n",
    "$$f_X(x) = f_X(x \\, / \\, A_1)P(A_1)+\\ldots + f_X(x \\, / \\, A_K)P(A_K)$$\n",
    "\n",
    "Podemos obtener una versión continua del teorema integrando $f_X(x \\, / \\, A) = \\frac{P(A \\, / \\, X=x)f_X(x)}{P(A)}$:\n",
    "\n",
    "$$P(A) = \\int_{-\\infty}^{+\\infty} P(A \\, / \\, X=x)f_X(x) dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Esperanza y momentos condicionados\n",
    "\n",
    "$$\\eta_{X/A} = E\\{X \\, / \\, A\\}=\\int_{-\\infty}^{\\infty}xf_X(x\\, / \\, A)dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
